{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5S1SQzKVcnR",
    "tags": []
   },
   "source": [
    "# Common code necessary to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 15:13:47.730522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_270002/2243617160.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available\")\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print(\"GPU is being used for the current session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after install package remember to restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: earthengine-api in /home/jinda/anaconda3/lib/python3.11/site-packages (0.1.379)\n",
      "Requirement already satisfied: google-cloud-storage in /home/jinda/anaconda3/lib/python3.11/site-packages (from earthengine-api) (2.13.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.12.1 in /home/jinda/anaconda3/lib/python3.11/site-packages (from earthengine-api) (2.108.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /home/jinda/anaconda3/lib/python3.11/site-packages (from earthengine-api) (2.23.4)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/jinda/anaconda3/lib/python3.11/site-packages (from earthengine-api) (0.1.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /home/jinda/anaconda3/lib/python3.11/site-packages (from earthengine-api) (0.22.0)\n",
      "Requirement already satisfied: requests in /home/jinda/anaconda3/lib/python3.11/site-packages (from earthengine-api) (2.31.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-api-python-client>=1.12.1->earthengine-api) (2.14.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-api-python-client>=1.12.1->earthengine-api) (4.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-auth>=1.4.1->earthengine-api) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-auth>=1.4.1->earthengine-api) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-auth>=1.4.1->earthengine-api) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/jinda/anaconda3/lib/python3.11/site-packages (from httplib2<1dev,>=0.9.2->earthengine-api) (3.0.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-cloud-storage->earthengine-api) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-cloud-storage->earthengine-api) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-cloud-storage->earthengine-api) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jinda/anaconda3/lib/python3.11/site-packages (from requests->earthengine-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jinda/anaconda3/lib/python3.11/site-packages (from requests->earthengine-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jinda/anaconda3/lib/python3.11/site-packages (from requests->earthengine-api) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jinda/anaconda3/lib/python3.11/site-packages (from requests->earthengine-api) (2023.11.17)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.61.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /home/jinda/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (4.25.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/jinda/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install earthengine-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 28 19:56:48 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 35%   48C    P8              17W / 250W |   6344MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX 1080 Ti     On  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 29%   41C    P8               9W / 250W |   4470MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce GTX 1080 Ti     On  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 29%   34C    P8               9W / 250W |   4234MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    199647      C   ...a1/T5/CS6140_Final/.venv/bin/python     6238MiB |\n",
      "|    0   N/A  N/A    239706    C+G   ...85695355,4430137353792653999,262144      101MiB |\n",
      "|    1   N/A  N/A    165780      C   ...a1/T5/CS6140_Final/.venv/bin/python      232MiB |\n",
      "|    1   N/A  N/A    199647      C   ...a1/T5/CS6140_Final/.venv/bin/python     4234MiB |\n",
      "|    2   N/A  N/A    199647      C   ...a1/T5/CS6140_Final/.venv/bin/python     4230MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5Relaa6YDtR",
    "outputId": "93fd4849-a42e-4b0c-bc60-f802e55a0831",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import os\n",
    "# import folium\n",
    "# import geemap\n",
    "from pprint import pprint\n",
    "# project = \"wildfire-feature-importance\"\n",
    "project = \"ee-my-char-test\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auth earth engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=J37O_4e9w5zcPAw1DN1kBnrLxI0m6qJBU4k_QSucunI&tc=gnDbuVlZYedpXnGPV8PzYOwOXmCY3n9AvnqxpWoukMU&cc=4x3JrEd4yZPLtrEBbAep_koWs1yBn4pBA0jNEliYmW0>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=J37O_4e9w5zcPAw1DN1kBnrLxI0m6qJBU4k_QSucunI&tc=gnDbuVlZYedpXnGPV8PzYOwOXmCY3n9AvnqxpWoukMU&cc=4x3JrEd4yZPLtrEBbAep_koWs1yBn4pBA0jNEliYmW0</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize(\n",
    "    project=project,\n",
    "    opt_url=\"https://earthengine-highvolume.googleapis.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AbortedError",
     "evalue": "All 10 retry attempts failed. The last failure: Error executing an HTTP request: libcurl code 60 meaning 'SSL peer certificate or SSH remote key was not OK', error details: SSL certificate problem: unable to get local issuer certificate\n\t when reading metadata of gs://tfds-data/dataset_info/mnist/3.0.1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jinda/Downloads/7980-ai-for-earth/ml (1).ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y523sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y523sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(\u001b[39m\"\u001b[39m\u001b[39mgs://tfds-data/dataset_info/mnist/3.0.1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/lib/io/file_io.py:290\u001b[0m, in \u001b[0;36mfile_exists_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determines whether a path exists or not.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[39m>>> with open(\"/tmp/x\", \"w\") as f:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m  errors.OpError: Propagates any errors reported by the FileSystem API.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m   _pywrap_file_io\u001b[39m.\u001b[39mFileExists(compat\u001b[39m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    291\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError:\n\u001b[1;32m    292\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAbortedError\u001b[0m: All 10 retry attempts failed. The last failure: Error executing an HTTP request: libcurl code 60 meaning 'SSL peer certificate or SSH remote key was not OK', error details: SSL certificate problem: unable to get local issuer certificate\n\t when reading metadata of gs://tfds-data/dataset_info/mnist/3.0.1"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.io.gfile.exists(\"gs://tfds-data/dataset_info/mnist/3.0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS6cWZ22dbu3"
   },
   "source": [
    "## Define visualization palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tTQbik0lQjbA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fire_vis = ['bbe029', '0a9501', '074b03']\n",
    "\n",
    "landcover_vis = {\n",
    "    \"Evergreen Needleleaf Forests\": \"05450a\",\n",
    "    \"Evergreen Broadleaf Forests\": \"086a10\",\n",
    "    \"Deciduous Needleleaf Forests\": \"54a708\",\n",
    "    \"Deciduous Broadleaf Forests\": \"78d203\",\n",
    "    \"Mixed Forests\": \"009900\",\n",
    "    \"Closed Shrublands\": \"c6b044\",\n",
    "    \"Open Shrublands\": \"dcd159\",\n",
    "    \"Woody Savannas\": \"dade48\",\n",
    "    \"Savannas\": \"fbff13\",\n",
    "    \"Grasslands\": \"b6ff05\",\n",
    "    \"Permanent Wetlands\": \"27ff87\",\n",
    "    \"Croplands\": \"c24f44\",\n",
    "    \"Urban and Built-up Lands\": \"a5a5a5\",\n",
    "    \"Cropland/Natural Vegetation Mosaics\": \"ff6d4c\",\\\n",
    "    \"Permanent Snow and Ice\": \"69fff8\",\n",
    "    \"Barren\": \"f9ffa4\",\n",
    "    \"Water Bodies\": \"1c0dff\",\n",
    "}\n",
    "\n",
    "temp_vis = [\n",
    "    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n",
    "    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n",
    "    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n",
    "    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n",
    "    'ff0000', 'de0101', 'c21301', 'a71001', '911003'\n",
    "  ]\n",
    "\n",
    "rain_vis = ['d8d8d8', '4addff', '5affa3', 'f2ff89', 'ff725c']\n",
    "\n",
    "\n",
    "ndvi_vis = [\n",
    "    'ffffff', 'ce7e45', 'df923d', 'f1b555', 'fcd163', '99b718', '74a901',\n",
    "    '66a000', '529400', '3e8601', '207401', '056201', '004c00', '023b01',\n",
    "    '012e01', '011d01', '011301'\n",
    "  ]\n",
    "\n",
    "\n",
    "# Global ALOS Landforms\n",
    "# topo_vis = [\n",
    "#     'red', 'red', 'blue', 'blue', 'blue', 'blue',\n",
    "#     'blue', 'blue', 'blue', 'blue', 'blue', 'blue',\n",
    "#     'blue', 'blue', 'green', 'green', 'green', 'green',\n",
    "#     'yellow', 'yellow', 'orange', 'red', 'red', 'red',\n",
    "#     'red', 'red', 'red', 'red', 'red'\n",
    "#   ]\n",
    "\n",
    "cdem_vis = ['0905ff', 'ffefc4', 'ffffff']\n",
    "\n",
    "windspeed_vis = ['ff00ff', '00ffff', 'ffff00']\n",
    "\n",
    "soil_moisture_info_vis = ['ff5733', '1eae98', 'a693bf']\n",
    "\n",
    "evapotranspiration_vis = ['0099cc', 'ffcc00', '9900cc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-4qjpOwddhD"
   },
   "source": [
    "## Define feature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hmXAp_QstHhV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FIRMS: Fire Information for Resource Management System\n",
    "# 2000-2023 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/FIRMS\n",
    "fire_map_info = {\n",
    "    'name':'FIRMS', \n",
    "    'band':'T21', \n",
    "    'min':325, \n",
    "    'max':400, \n",
    "    'vis':fire_vis, \n",
    "    'unmask':0,\n",
    "    'is_label':-1,\n",
    "}\n",
    "fire_class_band = 'is_fire'\n",
    "\n",
    "\n",
    "feature_list = []\n",
    "# Terra Land Surface Temperature and Emissivity Daily Global 1km\n",
    "# 2000-2023 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD11A1\n",
    "temp_map_info = {\n",
    "    'name':'MODIS/061/MOD11A1', \n",
    "    'band':'LST_Day_1km', \n",
    "    'min':13000.0, \n",
    "    'max':16500.0, \n",
    "    'vis':temp_vis}\n",
    "feature_list.append(temp_map_info)\n",
    "\n",
    "\n",
    "# TerraClimate: Monthly Climate and Climatic Water Balance for Global Terrestrial Surfaces, \n",
    "# Precipitation accumulation \n",
    "# -2022.12 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE\n",
    "rain_map_info = {\n",
    "    'name':'IDAHO_EPSCOR/TERRACLIMATE', \n",
    "    'band':'pr', \n",
    "    'min':0, \n",
    "    'max':2000, \n",
    "    'vis':rain_vis}\n",
    "feature_list.append(rain_map_info)\n",
    "\n",
    "\n",
    "# MCD12Q1.061 MODIS Land Cover Type Yearly Global 500m, \n",
    "# \tLand Cover Type 1: Annual International Geosphere-Biosphere Programme (IGBP) classification\n",
    "# -2022.1 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MCD12Q1\n",
    "landcover_map_info = {\n",
    "    'name':'MODIS/061/MCD12Q1', \n",
    "    'band':'LC_Type1', \n",
    "    'min':1.0, \n",
    "    'max':17.0, \n",
    "    'vis':landcover_vis,\n",
    "    'annual':True}\n",
    "feature_list.append(landcover_map_info)\n",
    "\n",
    "\n",
    "# MODIS Combined 16-Day NDVI\n",
    "# 2000-2022.1 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/MODIS_MCD43A4_006_NDVI\n",
    "ndvi_map_info = {\n",
    "    'name':'MODIS/MCD43A4_006_NDVI', \n",
    "    'band':'NDVI', \n",
    "    'min':0, \n",
    "    'max':1, \n",
    "    'vis':ndvi_vis}\n",
    "feature_list.append(ndvi_map_info)\n",
    "\n",
    "# The ALOS Landform dataset provides landform classes created by combining the Continuous Heat-Insolation Load Index (ALOS CHILI) and the multi-scale Topographic Position Index (ALOS mTPI) datasets. It is based on the 30m \"AVE\" band of JAXA's ALOS DEM (available in EE as JAXA/ALOS/AW3D30_V1_1).\n",
    "# 2006-01-24T00:00:00Z–2011-05-13T00:00:00\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD11A1\n",
    "# topo_map_info = {\n",
    "#     'name': 'CSP/ERGo/1_0/Global/ALOS_landforms',\n",
    "#     'band': 'constant',\n",
    "#     'min': 11, \n",
    "#     'max': 42, \n",
    "#     'vis': 'topo_vis',\n",
    "#     'unmask': -1,\n",
    "#     'topo': -1,\n",
    "# }\n",
    "# feature_list.append(topo_map_info)\n",
    "\n",
    "\n",
    "# The Canadian Digital Elevation Model (CDEM) is part of Natural Resources Canada's (NRCan) altimetry system and stems from the existing Canadian Digital Elevation Data (CDED). In these data, elevations can be either ground or reflective surface elevations.\n",
    "# 1945-01-01T00:00:00Z–2011-01-01T00:00:00 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/NRCan_CDEM#description\n",
    "cdem_map_info = {\n",
    "    'name': 'NRCan/CDEM',\n",
    "    'band': 'elevation',\n",
    "    'min': -226, \n",
    "    'max': 5944, \n",
    "    'vis': 'cdem_vis',\n",
    "    'no_time': -1,\n",
    "}\n",
    "feature_list.append(cdem_map_info)\n",
    "\n",
    "\n",
    "# TerraClimate: Monthly Climate and Climatic Water Balance for Global Terrestrial Surfaces\n",
    "# -2022.12 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE\n",
    "windspeed_map_info = {\n",
    "    'name':'IDAHO_EPSCOR/TERRACLIMATE', \n",
    "    'band':'vs', \n",
    "    'min':0, \n",
    "    'max':2923, \n",
    "    'vis':windspeed_vis}\n",
    "feature_list.append(windspeed_map_info)\n",
    "\n",
    "\n",
    "# TerraClimate: Monthly Climate and Climatic Water Balance for Global Terrestrial Surfaces\n",
    "# -2022.12 verified\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE\n",
    "soil_moisture_info = {\n",
    "    'name':'IDAHO_EPSCOR/TERRACLIMATE', \n",
    "    'band':'soil', \n",
    "    'min':0, \n",
    "    'max':8882, \n",
    "    'vis':soil_moisture_info_vis}\n",
    "feature_list.append(soil_moisture_info)\n",
    "\n",
    "\n",
    "# TerraClimate: Monthly Climate and Climatic Water Balance for Global Terrestrial Surfaces\n",
    "# -2022.12\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE\n",
    "evapotranspiration_info = {\n",
    "    'name':'IDAHO_EPSCOR/TERRACLIMATE', \n",
    "    'band':'aet', \n",
    "    'min':0, \n",
    "    'max':3140, \n",
    "    'vis':evapotranspiration_vis}\n",
    "feature_list.append(evapotranspiration_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "pprint(len(feature_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJKmYTagWLIr"
   },
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IgfBVpmszpIJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MONTH_SEASON = [[4,5,6],[7,8,9],[10,11,12]]\n",
    "NAME_SEASON = ['Spring', 'Summer', 'Fall']\n",
    "TRAIN_RANGE = [2018, 2019, 2020]\n",
    "PREDICT_RANGE = [2021]\n",
    "PRED_WINDOW = 2 #2 week prior to fire map to sample features\n",
    "FEATURE_COLUMN_NAMES = ['temp', 'rain', 'landcover', 'ndvi', 'cdem', 'windspeed', 'soil', 'evapotranspiration']\n",
    "\n",
    "train_temporal_gap = 1 # 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qvxo02VovW6I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "countries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n",
    "Canada = countries.filter(ee.Filter.eq('country_na', 'Canada')).geometry()\n",
    "America = countries.filter(ee.Filter.eq('country_na', 'United States')).geometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Jxv1N9PToVCz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Center in Vancouver\n",
    "center = [-123.1207, 49.2827]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gVwffjmYovaE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATCH_SIZE = 128\n",
    "SAMPLE_SCALE = 1000\n",
    "SAMPLE_POINT = 400\n",
    "SHARD_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eGb6ny9OyZt-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'wildfire-export'\n",
    "TRAINING_FOLDER = 'training_export'\n",
    "TEST_FOLDER = 'test_export'\n",
    "TRAINING_OUTPUT_PATH = f'gs://{BUCKET_NAME}/{TRAINING_FOLDER}/'\n",
    "TEST_OUTPUT_PATH = f'gs://{BUCKET_NAME}/{TEST_FOLDER}/'\n",
    "MODEL_SAVE_PATH = f'gs://{BUCKET_NAME}/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def process_img_value(img, map_info):\n",
    "#     unmask_value = map_info['min'] - (map_info['max'] - map_info['min'])*0.1\n",
    "#     return img.clamp(map_info['min'], map_info['max']).float().unmask(unmask_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUr6k6SqVrRT",
    "tags": []
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KM_jPoGiYDtU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_range_start_from_month(year, month, gap=1):\n",
    "    \"\"\"\n",
    "    Get a date range starting from a specified year and month and spanning a given number of months.\n",
    "\n",
    "    Args:\n",
    "    year (int): The starting year.\n",
    "    month (int): The starting month (1 to 12).\n",
    "    gap (int, optional): The number of months to span (default is 1).\n",
    "\n",
    "    Returns:|\n",
    "    tuple: A tuple containing the start and end dates as ee.Date objects.\n",
    "    \"\"\"\n",
    "    start = ee.Date(f'{year}-{month:02d}-01')\n",
    "    end = start.advance(gap, 'month')\n",
    "    return (start, end)\n",
    "\n",
    "\n",
    "def get_range_from_year(year):\n",
    "    \"\"\"\n",
    "    Get a date range for a given year.\n",
    "\n",
    "    Args:\n",
    "    year (int): The year for which you want to obtain a date range.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two strings representing the start and end dates of the year, e.g., ('year-01-01', 'year-12-31').\n",
    "    \"\"\"\n",
    "    return (f'{year}-01-01', f'{year}-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sum_maps(map_info, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Aggregate monthly precipitation into an annual sum.\n",
    "\n",
    "    Args:\n",
    "    map_info (dict): A dictionary containing information about the Earth Engine ImageCollection, including 'name' and 'band'.\n",
    "    start_date (str): The start date for filtering images (e.g., 'YYYY-MM-DD').\n",
    "    end_date (str): The end date for filtering images (e.g., 'YYYY-MM-DD').\n",
    "\n",
    "    Returns:\n",
    "    ee.Image: An Earth Engine image representing the annual sum of precipitation.\n",
    "\n",
    "    \"\"\"\n",
    "    collection = ee.ImageCollection(map_info['name']).filterDate(start_date, end_date).select(map_info['band'])\n",
    "    img = collection.reduce(ee.Reducer.sum()).select(map_info['band']+'_sum')\n",
    "    # pprint(img.getInfo())\n",
    "    return img.rename(map_info['band'])\n",
    "\n",
    "def median_maps(map_info, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calculate the median of maps within a specified date range.\n",
    "\n",
    "    Args:\n",
    "    map_info (dict): A dictionary containing information about the Earth Engine ImageCollection, including 'name' and 'band'.\n",
    "    start_date (str): The start date for filtering images (e.g., 'YYYY-MM-DD').\n",
    "    end_date (str): The end date for filtering images (e.g., 'YYYY-MM-DD').\n",
    "\n",
    "    Returns:\n",
    "    ee.Image: An Earth Engine image representing the median of maps within the specified date range.\n",
    "\n",
    "    \"\"\"\n",
    "    img = ee.ImageCollection(map_info['name']).filterDate(start_date, end_date).select(map_info['band']).median()\n",
    "    # pprint(img.getInfo())\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def max_maps(map_info, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calculate the max of maps within a specified date range.\n",
    "\n",
    "    Args:\n",
    "    map_info (dict): A dictionary containing information about the Earth Engine ImageCollection, including 'name' and 'band'.\n",
    "    start_date (str): The start date for filtering images (e.g., 'YYYY-MM-DD').\n",
    "    end_date (str): The end date for filtering images (e.g., 'YYYY-MM-DD').\n",
    "\n",
    "    Returns:\n",
    "    ee.Image: An Earth Engine image representing the max of maps within the specified date range.\n",
    "\n",
    "    \"\"\"\n",
    "    img = ee.ImageCollection(map_info['name']).filterDate(start_date, end_date).select(map_info['band']).max()\n",
    "    # pprint(img.getInfo())\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XB0FDpLDp3VH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_from_collection(map_info, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieve an image from a collection based on the specified map information, start date, and end date.\n",
    "\n",
    "    Args:\n",
    "    map_info (dict): A dictionary containing information about the Earth Engine ImageCollection.\n",
    "    start_date (str or ee.Date): The start date for filtering images (e.g., 'YYY-MM-DD' or an ee.Date object).\n",
    "    end_date (str or ee.Date): The end date for filtering images (e.g., 'YYYY-MM-DD' or an ee.Date object).\n",
    "\n",
    "    Returns:\n",
    "    ee.Image: An Earth Engine image representing the data from the specified collection and time period.\n",
    "    \n",
    "    \"\"\"\n",
    "    if map_info.get(\"is_label\"):\n",
    "        firms = max_maps(map_info, start_date, end_date).unmask(map_info['unmask']).divide(map_info['max'])\n",
    "        renamed_image = firms.ceil().byte().clamp(0, 1).rename(fire_class_band)\n",
    "        return renamed_image\n",
    "    \n",
    "    if map_info.get('topo'):\n",
    "        return ee.Image(topo_map_info['topo'])\n",
    "    \n",
    "    if map_info.get('no_time'):\n",
    "        img = ee.ImageCollection(cdem_map_info['name']).select(cdem_map_info['band']).median()\n",
    "        return img\n",
    "    \n",
    "    if map_info.get('annual'):\n",
    "        (start_date,end_date) = get_range_from_year(start_date.get('year').getInfo())\n",
    "        \n",
    "    if map_info.get('sum'):\n",
    "        return sum_maps(map_info, start_date, end_date).float()\n",
    "        # return sum_maps(map_info, start_date, end_date).float()\n",
    "        \n",
    "    return median_maps(map_info, start_date, end_date).float()\n",
    "    # return median_maps(map_info, start_date, end_date).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-NA--4Asl43b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_image(start_date, end_date) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Get a combined Earth Engine image from multiple collections for a specified time period.\n",
    "\n",
    "    Args:\n",
    "    start_date (str or ee.Date): The start date for filtering images (e.g., 'YYYY-MM-DD' or an ee.Date object).\n",
    "    end_date (str or ee.Date): The end date for filtering images (e.g., 'YYYY-MM-DD' or an ee.Date object).\n",
    "\n",
    "    Returns:\n",
    "    ee.Image: A combined Earth Engine image containing data from multiple collections within the specified time period.\n",
    "\n",
    "    \"\"\"\n",
    "    combined_image = get_img_from_collection(feature_list[0], start_date, end_date)\n",
    "    for i in range(1, len(feature_list)):\n",
    "        combined_image = ee.Image.cat([combined_image, get_img_from_collection(feature_list[i], start_date, end_date)])\n",
    "    return combined_image\n",
    "\n",
    "# print(get_input_image('2021-01-01', '2021-12-31').bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IsyKPJOKl-7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_label_image(start_date, end_date) -> ee.Image:\n",
    "    return get_img_from_collection(fire_map_info, start_date, end_date)\n",
    "# print(get_label_image('2021-01-01', '2021-12-31').bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5pz9uu44YDtV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Image', 'bands': [{'id': 'is_fire', 'data_type': {'type': 'PixelType', 'precision': 'int', 'min': 0, 'max': 1}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}]}\n"
     ]
    }
   ],
   "source": [
    "def test_band():\n",
    "    (start_date, end_date) = get_range_start_from_month(2021, 5, 1)\n",
    "    # print(start_date)\n",
    "    # print()\n",
    "    # print(end_date)\n",
    "    \n",
    "    img = get_img_from_collection(fire_map_info, start_date, end_date)\n",
    "    # print(img.getInfo())\n",
    "    \n",
    "    # img = ee.Image(topo_map_info['topo'])\n",
    "    \n",
    "    # img = ee.ImageCollection(fire_map_info['name'])\\\n",
    "    #     .filterDate(start_date,end_date)\\\n",
    "    #     .select(topo_map_info['band'])\\\n",
    "    #     .median()\n",
    "    \n",
    "    print(img.getInfo())\n",
    "    # print(img.getInfo())\n",
    "    # print(img.bandNames().getInfo())\n",
    "test_band()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwShjgEm1H_p",
    "tags": []
   },
   "source": [
    "## Visualize maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SlqTJ_MYDtV",
    "outputId": "2bfffe72-4476-4a3e-ba9b-cc131cc16950",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !jupyter trust Natural_Factors_Impacting_Carbon_Sequestration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2021-01-01', '2021-12-31')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_year = 2021\n",
    "(visualize_start_date, visualize_end_date) = get_range_from_year(visualize_year)\n",
    "(visualize_start_date, visualize_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "id": "KVx1NR_r1Gd8",
    "outputId": "f98ff091-eb81-41e8-a836-2550175b453b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tile_layer(map_info):\n",
    "  if map_info.get('sum'):\n",
    "    image = sum_maps(map_info, visualize_start_date, visualize_end_date)\n",
    "  elif map_info.get(\"topo\"):\n",
    "    image = ee.Image(topo_map_info['topo'])\n",
    "  else:\n",
    "    image = median_maps(map_info, visualize_start_date, visualize_end_date)\n",
    "  image=image.clip(Canada)\n",
    "  vis_params = {\n",
    "    'bands': [map_info['band']],\n",
    "    'min': map_info['min'],\n",
    "    'max': map_info['max'],\n",
    "    }\n",
    "  if isinstance(map_info['vis'], dict):\n",
    "    vis_params['palette'] = list(map_info['vis'].values())\n",
    "  else:\n",
    "    vis_params['palette'] = map_info['vis']\n",
    "  mapid = image.getMapId(vis_params)\n",
    "  return folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    overlay=True,\n",
    "    name=map_info['band'],\n",
    "  )\n",
    "\n",
    "# map = folium.Map(location=(center[1], center[0]), zoom_start=7)\n",
    "# for map_info in feature_list:\n",
    "#   get_tile_layer(map_info).add_to(map)\n",
    "\n",
    "# get_tile_layer(fire_map_info).add_to(map)\n",
    "\n",
    "# folium.LayerControl().add_to(map)\n",
    "# map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTqRlT_1hz5l",
    "tags": []
   },
   "source": [
    "## Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5EvsWZo-YDtV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Each season samples from training year\n",
    "def get_combined_image(is_training: bool):\n",
    "    \"\"\"\n",
    "    Retrieve training data for multiple seasons, each sampling from training years.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing seasonal data with Earth Engine images, where each season is a list of images from training years and months\n",
    "    \n",
    "    \"\"\"\n",
    "    year_range = TRAIN_RANGE if is_training else PREDICT_RANGE\n",
    "    combine_image_list = [] # Store 3 elements: spring, summer, fall\n",
    "    for season in MONTH_SEASON:\n",
    "      season_list = [] # Store all months for this season from all training years\n",
    "      for month in season:\n",
    "        for year in year_range:\n",
    "          time_range=get_range_start_from_month(year, month)\n",
    "          input_image = get_input_image(time_range[0], time_range[1])\n",
    "          labels_image = get_label_image(time_range[0].advance(PRED_WINDOW, 'week'), time_range[1].advance(PRED_WINDOW, 'week'))\n",
    "          combine_image = ee.Image.cat([input_image, labels_image])\n",
    "          season_list.append(combine_image)\n",
    "      combine_image_list.append(season_list)\n",
    "    return combine_image_list\n",
    "\n",
    "combine_image_list = get_combined_image(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_fire():\n",
    "    Map = geemap.Map(center=[47.295, -119.086], zoom=6)\n",
    "    combine_image_list = get_combined_image(True)\n",
    "    # Create an empty list to store the test results\n",
    "    test_results = []\n",
    "\n",
    "    for season in combine_image_list:\n",
    "        for month_image in season:\n",
    "            test_img = month_image\n",
    "            t21_test = test_img.select(fire_class_band)\n",
    "\n",
    "            region = ee.Geometry.Point([-123.1207, 49.2827]).buffer(100009)\n",
    "            num_samples = 100 \n",
    "\n",
    "            samples = t21_test.stratifiedSample(\n",
    "                region=region, \n",
    "                numPoints=50,\n",
    "                classBand=fire_class_band,\n",
    "                scale=1000)\n",
    "            sample_values = samples.aggregate_array(fire_class_band)\n",
    "            lst = sample_values.getInfo()\n",
    "\n",
    "            # contains_non_zero = any(element != 0 for element in lst)\n",
    "            value_counts = {value: lst.count(value) for value in set(lst)}\n",
    "\n",
    "            test_results.append(value_counts)        \n",
    "    print(test_results)\n",
    "    \n",
    "# test_fire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Y-6oh4ROAhaG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "def sample_points(\n",
    "    region: ee.Geometry, num_points: int, scale: int, is_training: bool\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample points from Earth Engine images and export them to Cloud Storage in TFRecord format.\n",
    "\n",
    "    Args:\n",
    "    region (ee.Geometry): An Earth Engine geometry representing the region of interest.\n",
    "    num_points (int): The total number of points to sample.\n",
    "    scale (int): The scale at which to sample the points.\n",
    "    \"\"\"\n",
    "    random.seed(int(time.time()))\n",
    "\n",
    "    sample_shards = 10\n",
    "    num_points /= sample_shards\n",
    "    combine_image_list = get_combined_image(is_training)\n",
    "    export_folder = TRAINING_FOLDER if is_training else TEST_FOLDER\n",
    "    for i, season in enumerate(combine_image_list):\n",
    "        for j, month_image in enumerate(season):\n",
    "            collection = ee.FeatureCollection([])\n",
    "            for k in range(sample_shards):\n",
    "                fc = month_image.stratifiedSample(\n",
    "                    numPoints=num_points,\n",
    "                    classBand=fire_class_band,\n",
    "                    region=region,\n",
    "                    scale=scale,\n",
    "                    seed=int(random.random() * time.time())+i+j+k\n",
    "                )\n",
    "                collection = collection.merge(fc)\n",
    "                \n",
    "            task = ee.batch.Export.table.toCloudStorage(\n",
    "                collection = collection,\n",
    "                description = f'{NAME_SEASON[i]}-{j}',\n",
    "                bucket = BUCKET_NAME,\n",
    "                fileNamePrefix = f'{export_folder}/{NAME_SEASON[i]}-{j}',\n",
    "                fileFormat = 'TFRecord'\n",
    "            )\n",
    "            # task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_points(Canada, SAMPLE_POINT, SAMPLE_SCALE, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_points(Canada, SAMPLE_POINT, SAMPLE_SCALE, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eq8MGNwOYhOg"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ZfoOnAvpYDtV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LST_Day_1km': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'pr': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'LC_Type1': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'NDVI': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'elevation': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'vs': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'soil': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'aet': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'is_fire': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "NUM_INPUTS = len(feature_list)\n",
    "LABEL = fire_class_band\n",
    "BANDS = []\n",
    "for feature in feature_list:\n",
    "  BANDS += [feature['band']]\n",
    "FEATURE_NAMES = BANDS + [LABEL]\n",
    "\n",
    "PATCH_SHAPE = [PATCH_SIZE, PATCH_SIZE]\n",
    "columns = [tf.io.FixedLenFeature(shape=[], dtype=tf.float32) for k in FEATURE_NAMES]\n",
    "# columns.append(tf.io.FixedLenFeature(shape=[], dtype=tf.int64))\n",
    "FEATURES_DICT = dict(zip(FEATURE_NAMES, columns))\n",
    "print(FEATURES_DICT)\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13000.0, 16500.0],\n",
      " [0, 2000],\n",
      " [1.0, 17.0],\n",
      " [0, 1],\n",
      " [-226, 5944],\n",
      " [0, 2923],\n",
      " [0, 8882],\n",
      " [0, 3140]]\n"
     ]
    }
   ],
   "source": [
    "fixed_range = []\n",
    "for feature in feature_list:\n",
    "    feature_range = [feature['min'], feature['max']]\n",
    "    fixed_range.append(feature_range)\n",
    "# fixed_range.append([0,1]) # after divide and byte new bands is 0, 1\n",
    "normed_range = [[-1,1]]\n",
    "pprint(fixed_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yKImCg_YDtV",
    "outputId": "9a374e5d-3b11-4c11-a3e8-6916acb5674f"
   },
   "outputs": [],
   "source": [
    "def normalize_fixed(x, current_range, normed_range):\n",
    "    \"\"\"\n",
    "    Normalize a value within a specific range to another range.\n",
    "\n",
    "    Args:\n",
    "    x: The value to be normalized.\n",
    "    current_range: A list containing the minimum and maximum values of the current range.\n",
    "    normed_range: A list containing the minimum and maximum values of the target range.\n",
    "\n",
    "    Returns:\n",
    "    The normalized value within the target range.\n",
    "    \"\"\"\n",
    "    current_min, current_max = current_range[:,0], current_range[:,1]\n",
    "    normed_min, normed_max = normed_range[:,0], normed_range[:,1]\n",
    "    x_normed = (x - current_min) / (current_max - current_min)\n",
    "    x_normed = x_normed * (normed_max - normed_min) + normed_min\n",
    "    return x_normed\n",
    "\n",
    "# normalize_fixed(np.array([5.0,5.0]),np.array([[0.0,10.0],[0.0,10.0]]), np.array([[0.0,1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLPUBstCZAYx"
   },
   "source": [
    "## Parse tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prettytable import PrettyTable\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# def visualize_dataset(dataset):\n",
    "#     dataset_len = dataset.reduce(np.int64(0), lambda x, _: x + 1).numpy()\n",
    "#     print(f'dataset len: {dataset_len}')\n",
    "#     ds_numpy = tfds.as_numpy(dataset)\n",
    "#     x = PrettyTable()\n",
    "#     # x.field_names = [key for key in FEATURE_NAMES]\n",
    "#     for row in ds_numpy:\n",
    "#         x.add_row(row)\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_balance_dataset(dataset):\n",
    "    cnt_0 = 0\n",
    "    cnt_1 = 0\n",
    "    for inputs, label in dataset.as_numpy_iterator():\n",
    "        cnt_0 += (1 if label == 0 else 0)\n",
    "        cnt_1 += (1 if label == 1 else 0)\n",
    "    \n",
    "    print(f'cnt 0={cnt_0}, cnt_1={cnt_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5kwqeres-X_",
    "outputId": "df96ef7a-cbb3-466d-828f-54862d8600dd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "    \"\"\"The parsing function.\n",
    "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "    Args:\n",
    "    example_proto: a serialized Example.\n",
    "    Returns:\n",
    "    A dictionary of tensors, keyed by feature name.\n",
    "    \"\"\"\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "    # print(f'Parsed features: {parsed_features}')\n",
    "    return parsed_features\n",
    "\n",
    "def to_tuple(inputs, deb=True):\n",
    "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
    "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
    "    Args:\n",
    "    inputs: A dictionary of tensors, keyed by feature name.\n",
    "    Returns:\n",
    "    A tuple of (inputs, outputs).\n",
    "    \"\"\"\n",
    "    feature_list = [inputs.get(key) for key in BANDS]\n",
    "    feature_stacked = tf.stack(feature_list, axis=0)\n",
    "\n",
    "    # Convert from CHW to HWC\n",
    "    # See https://caffe2.ai/docs/tutorial-image-pre-processing.html\n",
    "    # feature_stacked = tf.transpose(feature_stacked, [1, 2, 0])\n",
    "    feature_stacked = normalize_fixed(feature_stacked, np.array(fixed_range, dtype=\"float32\"), np.array(normed_range, dtype=\"float32\"))\n",
    "    \n",
    "    label = inputs.get(LABEL)\n",
    "    label = 1 if label > 0.5 else 0\n",
    "    # label_list = tf.transpose(label_list, [1, 2, 0])\n",
    "    \n",
    "    return feature_stacked, label\n",
    "\n",
    "def get_dataset(pattern):\n",
    "    \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
    "    Get all the files matching the pattern, parse and convert to tuple.\n",
    "    Args:\n",
    "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
    "    Returns:\n",
    "    A tf.data.Dataset\n",
    "    \"\"\"\n",
    "    print(pattern)\n",
    "    glob = tf.io.gfile.glob(pattern)\n",
    "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "    dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5I3MzlHSgNay",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://wildfire-export/training_export/Spring*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 15:16:32.294709: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "ename": "AbortedError",
     "evalue": "All 10 retry attempts failed. The last failure: Error executing an HTTP request: libcurl code 60 meaning 'SSL peer certificate or SSH remote key was not OK', error details: SSL certificate problem: unable to get local issuer certificate\n\t when reading gs://wildfire-export/training_export",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jinda/Downloads/7980-ai-for-earth/ml (1).ipynb Cell 54\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_spring \u001b[39m=\u001b[39m get_dataset(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mTRAINING_OUTPUT_PATH\u001b[39m}\u001b[39;00m\u001b[39mSpring*\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/jinda/Downloads/7980-ai-for-earth/ml (1).ipynb Cell 54\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mGet all the files matching the pattern, parse and convert to tuple.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mA tf.data.Dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(pattern)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m glob \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mglob(pattern)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTFRecordDataset(glob, compression_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGZIP\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jinda/Downloads/7980-ai-for-earth/ml%20%281%29.ipynb#Y104sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(parse_tfrecord, num_parallel_calls\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/lib/io/file_io.py:441\u001b[0m, in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns a list of files that match the given pattern(s).\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \n\u001b[1;32m    389\u001b[0m \u001b[39mThe patterns are defined as strings. Supported patterns are defined\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39m  errors.NotFoundError: If pattern to be matched is an invalid directory.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pattern, six\u001b[39m.\u001b[39mstring_types):\n\u001b[1;32m    438\u001b[0m   \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    439\u001b[0m       \u001b[39m# Convert the filenames to string from bytes.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m       compat\u001b[39m.\u001b[39mas_str_any(matching_filename)\n\u001b[0;32m--> 441\u001b[0m       \u001b[39mfor\u001b[39;00m matching_filename \u001b[39min\u001b[39;00m _pywrap_file_io\u001b[39m.\u001b[39mGetMatchingFiles(\n\u001b[1;32m    442\u001b[0m           compat\u001b[39m.\u001b[39mas_bytes(pattern))\n\u001b[1;32m    443\u001b[0m   ]\n\u001b[1;32m    444\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m   \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    446\u001b[0m       \u001b[39m# Convert the filenames to string from bytes.\u001b[39;00m\n\u001b[1;32m    447\u001b[0m       compat\u001b[39m.\u001b[39mas_str_any(matching_filename)  \u001b[39m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m           compat\u001b[39m.\u001b[39mas_bytes(single_filename))\n\u001b[1;32m    451\u001b[0m   ]\n",
      "\u001b[0;31mAbortedError\u001b[0m: All 10 retry attempts failed. The last failure: Error executing an HTTP request: libcurl code 60 meaning 'SSL peer certificate or SSH remote key was not OK', error details: SSL certificate problem: unable to get local issuer certificate\n\t when reading gs://wildfire-export/training_export"
     ]
    }
   ],
   "source": [
    "dataset_spring = get_dataset(f'{TRAINING_OUTPUT_PATH}Spring*')\n",
    "# test_balance_dataset(dataset_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_summer = get_dataset(f'{TRAINING_OUTPUT_PATH}Summer*')\n",
    "dataset_fall = get_dataset(f'{TRAINING_OUTPUT_PATH}Fall*')\n",
    "# test_balance_dataset(dataset_summer)\n",
    "# test_balance_dataset(dataset_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sL79WsRF5XjZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_EVAL_RATIO = 80  # percent for training, the rest for testing\n",
    "\n",
    "def split_dataset(dataset: tf.data.Dataset, batch_size) -> tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Splits a TensorFlow dataset into training and evaluation datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): The input dataset to be split.\n",
    "        batch_size (int): The batch size for creating batches of data.\n",
    "\n",
    "    Returns:\n",
    "        tuple[tf.data.Dataset, tf.data.Dataset]: A tuple containing the training and evaluation datasets.\n",
    "    \"\"\"\n",
    "    dataset_shuffle = dataset.shuffle(1)\n",
    "    indexed_dataset = dataset_shuffle.enumerate()  # add an index to each example\n",
    "    train_dataset = (\n",
    "        indexed_dataset.filter(lambda i, _: i % 100 <= TRAIN_EVAL_RATIO)\n",
    "        .map(lambda _, data: data)  # remove index\n",
    "        .shuffle(batch_size * 8)  # randomize the examples for the batches\n",
    "        .batch(batch_size)  # batch randomized examples\n",
    "    )\n",
    "    eval_dataset = (\n",
    "        indexed_dataset.filter(lambda i, _: i % 100 > TRAIN_EVAL_RATIO)\n",
    "        .map(lambda _, data: data, num_parallel_calls=tf.data.AUTOTUNE)  # remove index\n",
    "        .batch(batch_size)  # batch the parsed examples, no need to shuffle\n",
    "    )\n",
    "    return (train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_train_eval_label(dataset, dataset_name):\n",
    "    num_0 = 0\n",
    "    num_1 = 0\n",
    "    for element in dataset.as_numpy_iterator():\n",
    "        num_1 += np.count_nonzero(element[1])\n",
    "        num_0 += len(element[1]) - np.count_nonzero(element[1])\n",
    "        \n",
    "    print(f'{dataset_name} count_0 = {num_0}, count_1 = {num_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_all_seasons = [dataset_spring, dataset_summer, dataset_fall]\n",
    "train_dataset_list = []\n",
    "eval_dataset_list = []\n",
    "\n",
    "for dataset in dataset_all_seasons:\n",
    "    (train_season, eval_season) = split_dataset(dataset, batch_size=32)\n",
    "    train_dataset_list.append(train_season)    \n",
    "    eval_dataset_list.append(eval_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_sanity_check():\n",
    "    for train_season, eval_season in zip(train_dataset_list, eval_dataset_list):\n",
    "        verify_train_eval_label(train_season, 'train')\n",
    "        verify_train_eval_label(eval_season, 'eval')\n",
    "# balance_sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Convert dataset to dataframe for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_dataset_to_dataframe(dataset):\n",
    "    \"\"\"\n",
    "    Converts a TensorFlow dataset to a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    dataset (tf.data.Dataset): A TensorFlow dataset containing feature-label pairs.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A Pandas DataFrame with feature and label columns. \n",
    "    \"\"\"\n",
    "    data_dict = {'features': [], 'labels': []}\n",
    "    \n",
    "    for x, y in dataset:\n",
    "        features = x.numpy()\n",
    "        labels = y.numpy()\n",
    "        \n",
    "        data_dict['features'].extend(features)\n",
    "        data_dict['labels'].extend(labels)\n",
    "    \n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    labels_df = df['labels']\n",
    "    df = pd.DataFrame(df['features'].to_list(), columns=FEATURE_COLUMN_NAMES)\n",
    "    df['labels'] = labels_df                              \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = [convert_dataset_to_dataframe(dataset) for dataset in train_dataset_list]\n",
    "eval_df = [convert_dataset_to_dataframe(dataset) for dataset in eval_dataset_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[-1]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsMdIhKbZHLj",
    "tags": []
   },
   "source": [
    "## Define plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3f5_TD00XwU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history, metric_name, plot_title):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history.history[metric_name], label=f'training {metric_name}')\n",
    "    plt.plot(history.history[f'val_{metric_name}'], label=f'validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{metric_name}')\n",
    "    plt.title(f'Training and Validation {plot_title} {metric_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function to save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "def save_model_ann(model_list, model_name):\n",
    "    for i, model in enumerate(model_list):\n",
    "        model.save(f\"{MODEL_SAVE_PATH}{model_name}/{NAME_SEASON[i]}\")\n",
    "        \n",
    "def load_model_ann(model_name, seasons):\n",
    "    loaded_models = []\n",
    "\n",
    "    for season in seasons:\n",
    "        print(f\"loading {season}\")\n",
    "        model_path = f\"{MODEL_SAVE_PATH}{model_name}/{season}\"\n",
    "        loaded_model = keras.models.load_model(model_path)\n",
    "        loaded_models.append(loaded_model)\n",
    "\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function to save and load metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_metrics(metrics, filename):\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(metrics, file)\n",
    "\n",
    "def load_metrics(filename):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kysuUEjLfPc",
    "tags": []
   },
   "source": [
    "## Mia - ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.metrics as metrics\n",
    "import tensorflow.keras.optimizers as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metric_names = ['Accuracy', 'Recall', 'AUC', 'FalseNegatives']\n",
    "def plot_model_history(history, plot_title):\n",
    "    for metric_name in all_metric_names:\n",
    "        plot_metrics(history, metric_name, plot_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ann_model(model, train_dataset, eval_dataset, epochs = 10):\n",
    "    # Compile Model\n",
    "    model.compile(optimizer = 'adam', metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name=all_metric_names[0]), \n",
    "        tf.keras.metrics.Recall(name=all_metric_names[1]),\n",
    "        tf.keras.metrics.AUC(name=all_metric_names[2]),\n",
    "        tf.keras.metrics.FalseNegatives(name=all_metric_names[3])], \n",
    "        loss ='binary_crossentropy',\n",
    "        )\n",
    "    # Train Model\n",
    "    history = model.fit(train_dataset, validation_data = eval_dataset, epochs = epochs)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ANN baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVzXM8BTlqHt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ann_baseline_model():\n",
    "  model = tf.keras.Sequential()\n",
    "  # input layer + 1st hidden layer\n",
    "  model.add(layers.Dense(32, input_dim=NUM_INPUTS, activation='relu', name='Dense1'))\n",
    "  # hidden layer\n",
    "  model.add(layers.Dense(16, activation='relu', name='Dense2'))\n",
    "  # output layer\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid', name='Output'))\n",
    "  model.summary()\n",
    "  tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ann_baseline_list = []\n",
    "for i, (train_dataset, eval_dataset) in enumerate(zip(train_dataset_list, eval_dataset_list)):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_ann_baseline, history_baseline = train_ann_model(get_ann_baseline_model(), train_dataset, eval_dataset)\n",
    "    model_ann_baseline_list.append(model_ann_baseline)\n",
    "    plot_model_history(history_baseline, NAME_SEASON[i])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model_ann(model_ann_baseline_list, \"model_ann_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN deeper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ann_deeper_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    init_filter = 128\n",
    "    cur_filter = init_filter/2\n",
    "    cur_layer = 1\n",
    "    # input layer + 1st hidden layer\n",
    "    model.add(layers.Dense(init_filter, input_dim=NUM_INPUTS, activation='relu', name='Dense0'))\n",
    "    # hidden layer\n",
    "    while(cur_layer < 5):\n",
    "        model.add(layers.Dense(cur_filter, activation='relu', name=f'Dense{cur_layer}'))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        # cur_filter /= 2\n",
    "        cur_layer += 1\n",
    "    # output layer\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid', name='Output'))\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# get_ann_deeper_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ann_deep_list = []\n",
    "for i, (train_dataset, eval_dataset) in enumerate(zip(train_dataset_list, eval_dataset_list)):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model_ann_deep, history_ann_deep = train_ann_model(get_ann_deeper_model(), train_dataset, eval_dataset, 10)\n",
    "    model_ann_deep_list.append(model_ann_deep)\n",
    "    plot_model_history(history_ann_deep, NAME_SEASON[i])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_ann(model_ann_deep_list, \"model_ann_deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TabTransformer https://keras.io/examples/structured_data/tabtransformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydot[and-cuda] --user\n",
    "# !pip install graphviz[and-cuda] --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_experiment(\n",
    "#     model,\n",
    "#     train_dataset,\n",
    "#     validation_dataset,\n",
    "#     num_epochs,\n",
    "#     learning_rate,\n",
    "#     weight_decay,\n",
    "# ):\n",
    "\n",
    "#     optimizer = optimizers.AdamW(\n",
    "#         learning_rate=learning_rate, weight_decay=weight_decay\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=keras.losses.BinaryCrossentropy(),\n",
    "#         metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "#     )\n",
    "\n",
    "#     print(\"Start training the model...\")\n",
    "#     history = model.fit(\n",
    "#         train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "#     )\n",
    "#     print(\"Model training finished\")\n",
    "\n",
    "#     _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "#     print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model_inputs():\n",
    "#     inputs = {}\n",
    "#     for feature_name in BANDS:\n",
    "#         inputs[feature_name] = layers.Input(\n",
    "#                     name=feature_name, shape=(), dtype=tf.float32)   \n",
    "#     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_inputs(inputs):\n",
    "#     encoded_categorical_feature_list = []\n",
    "#     numerical_feature_list = []\n",
    "\n",
    "#     for feature_name in inputs:\n",
    "#         # Use the numerical features as-is.\n",
    "#         numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "#         numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "#     return layers.concatenate(\n",
    "#         encoded_categorical_feature_list + numerical_feature_list\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "#     print(f'hidden_units={hidden_units} ')\n",
    "#     mlp_layers = []\n",
    "#     for units in hidden_units:\n",
    "#         mlp_layers.append(normalization_layer),\n",
    "#         mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "#         mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "#     return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING_RATE = 0.001\n",
    "# WEIGHT_DECAY = 0.0001\n",
    "# DROPOUT_RATE = 0.2\n",
    "# NUM_EPOCHS = 10\n",
    "\n",
    "# NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "# NUM_HEADS = 4  # Number of attention heads.\n",
    "# EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "# MLP_HIDDEN_UNITS_FACTORS = [\n",
    "#     2,\n",
    "#     1,\n",
    "# ]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "# NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_baseline_model(\n",
    "#     num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "# ):\n",
    "\n",
    "#     # Create model inputs.\n",
    "#     inputs = create_model_inputs()\n",
    "#     # encode features.\n",
    "#     features = encode_inputs(\n",
    "#         inputs\n",
    "#     )\n",
    "#     # Compute Feedforward layer units.\n",
    "#     feedforward_units = [features.shape[-1]]\n",
    "#     print(features.shape)\n",
    "#     # Create several feedforwad layers with skip connections.\n",
    "#     for layer_idx in range(num_mlp_blocks):\n",
    "#         features = create_mlp(\n",
    "#             hidden_units=feedforward_units,\n",
    "#             dropout_rate=dropout_rate,\n",
    "#             activation=keras.activations.gelu,\n",
    "#             normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "#             name=f\"feedforward_{layer_idx}\",\n",
    "#         )(features)\n",
    "\n",
    "#     # Compute MLP hidden_units.\n",
    "#     mlp_hidden_units = [\n",
    "#         factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "#     ]\n",
    "#     # Create final MLP.\n",
    "#     features = create_mlp(\n",
    "#         hidden_units=mlp_hidden_units,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         activation=keras.activations.selu,\n",
    "#         normalization_layer=layers.BatchNormalization(),\n",
    "#         name=\"MLP\",\n",
    "#     )(features)\n",
    "\n",
    "#     # Add a sigmoid as a binary classifer.\n",
    "#     outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_baseline_model = create_baseline_model(\n",
    "#     num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "#     mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "# )\n",
    "\n",
    "# print(\"Total model weights:\", transformer_baseline_model.count_params())\n",
    "# keras.utils.plot_model(transformer_baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_transformer_baseline = run_experiment(\n",
    "#     model=transformer_baseline_model,\n",
    "#     train_dataset=train_dataset_list[0],\n",
    "#     validation_dataset=eval_dataset_list[0],\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJO2Oz1_Lkr-",
    "tags": []
   },
   "source": [
    "## Huizi - RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1HcWfUQcuSG"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2PrcH-kcmE9"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store the metrics for all seasons\n",
    "all_metrics = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1-Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "# Compute metrics for each season\n",
    "for season, train_data, eval_data in zip(SEASONS, train_df, eval_df):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_eval = eval_data[\"labels\"]\n",
    "\n",
    "    # Training Random Forest model\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_eval)\n",
    "    y_pred_proba = rf.predict_proba(X_eval)[:, 1]  # assuming binary classification\n",
    "\n",
    "    # Computing metrics\n",
    "    accuracy = accuracy_score(y_eval, y_pred)\n",
    "    precision = precision_score(y_eval, y_pred, average='macro')\n",
    "    recall = recall_score(y_eval, y_pred, average='macro')\n",
    "    f1 = f1_score(y_eval, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_eval, y_pred_proba)\n",
    "\n",
    "    # Storing metrics\n",
    "    all_metrics[\"Season\"].append(season)\n",
    "    all_metrics[\"Accuracy\"].append(accuracy)\n",
    "    all_metrics[\"Precision\"].append(precision)\n",
    "    all_metrics[\"Recall\"].append(recall)\n",
    "    all_metrics[\"F1-Score\"].append(f1)\n",
    "    all_metrics[\"AUC\"].append(auc)\n",
    "\n",
    "# Convert metrics to DataFrame for presentation\n",
    "metrics_table = pd.DataFrame(all_metrics)\n",
    "metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k31O2uyS9faH"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Seasons for x-axis\n",
    "x = metrics_table['Season']\n",
    "\n",
    "# Plotting each metric as a line\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    plt.plot(x, metrics_table[metric], marker='o', label=metric)\n",
    "\n",
    "# Setting the title, labels, and legend\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics Across Seasons')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg-oS2Zt80PO"
   },
   "outputs": [],
   "source": [
    "# 1. Feature Importance\n",
    "from sklearn.tree import plot_tree\n",
    "import itertools\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "season_feature_importances = {}\n",
    "rf_models = []\n",
    "\n",
    "for season, train_data in zip(SEASONS, train_df):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_models.append(rf)\n",
    "    \n",
    "    # Extract feature importance\n",
    "    season_feature_importances[season] = rf.feature_importances_\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "feature_importance_df = pd.DataFrame(season_feature_importances, index=X_train.columns)\n",
    "\n",
    "for season in SEASONS:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance_df[season].sort_values().plot(kind='barh', title=f'Feature Importance for {season}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# Plotting\n",
    "feature_importance_df.plot(kind='bar', figsize=(15, 7))\n",
    "plt.title('Feature Importance across Seasons')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.legend(title='Season')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter distributions rather than a fixed set of values per hyperparameter\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 200, 500],\n",
    "    'max_depth': [None, 10, 15, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "best_params_per_season = {}\n",
    "\n",
    "# # Hyperparameter tuning for each season using RandomizedSearchCV\n",
    "# for season, train_data in zip(SEASONS, train_df):\n",
    "#     X_train = train_data.drop(columns=\"labels\")\n",
    "#     y_train = train_data[\"labels\"]\n",
    "    \n",
    "#     rf = RandomForestClassifier()\n",
    "#     random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "#                                        n_iter=50, cv=3, n_jobs=-1, verbose=2, scoring='accuracy', random_state=42)\n",
    "#     random_search.fit(X_train, y_train)\n",
    "    \n",
    "#     best_params_per_season[season] = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_per_season = {'Spring': {'n_estimators': 100,\n",
    "  'min_samples_split': 2,\n",
    "  'min_samples_leaf': 1,\n",
    "  'max_depth': 20,\n",
    "  'bootstrap': True},\n",
    " 'Summer': {'n_estimators': 200,\n",
    "  'min_samples_split': 5,\n",
    "  'min_samples_leaf': 2,\n",
    "  'max_depth': 15,\n",
    "  'bootstrap': True},\n",
    " 'Fall': {'n_estimators': 50,\n",
    "  'min_samples_split': 2,\n",
    "  'min_samples_leaf': 2,\n",
    "  'max_depth': 10,\n",
    "  'bootstrap': False}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models using the best parameters for each season\n",
    "tuned_all_metrics = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1-Score\": [],\n",
    "    \"AUC\": []\n",
    "}\n",
    "\n",
    "tuned_feature_importances = {}\n",
    "\n",
    "for season, train_data, eval_data in zip(SEASONS, train_df, eval_df):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_eval = eval_data[\"labels\"]\n",
    "    \n",
    "    rf = RandomForestClassifier(**best_params_per_season[season])\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_eval)\n",
    "    y_pred_proba = rf.predict_proba(X_eval)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_eval, y_pred)\n",
    "    precision = precision_score(y_eval, y_pred, average='macro')\n",
    "    recall = recall_score(y_eval, y_pred, average='macro')\n",
    "    f1 = f1_score(y_eval, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_eval, y_pred_proba)\n",
    "    \n",
    "    tuned_all_metrics[\"Season\"].append(season)\n",
    "    tuned_all_metrics[\"Accuracy\"].append(accuracy)\n",
    "    tuned_all_metrics[\"Precision\"].append(precision)\n",
    "    tuned_all_metrics[\"Recall\"].append(recall)\n",
    "    tuned_all_metrics[\"F1-Score\"].append(f1)\n",
    "    tuned_all_metrics[\"AUC\"].append(auc)\n",
    "\n",
    "    # Store feature importances\n",
    "    tuned_feature_importances[season] = rf.feature_importances_\n",
    "\n",
    "# Display metrics\n",
    "tuned_metrics_table = pd.DataFrame(tuned_all_metrics)\n",
    "print(tuned_metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the performance metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "x = tuned_metrics_table['Season']\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    plt.plot(x, tuned_metrics_table[metric], marker='o', label=metric)\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Tuned Performance Metrics Across Seasons')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importances after tuning\n",
    "tuned_feature_importance_df = pd.DataFrame(tuned_feature_importances, index=X_train.columns)\n",
    "for season in SEASONS:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    tuned_feature_importance_df[season].sort_values().plot(kind='barh', title=f'Tuned Feature Importance for {season}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all together\n",
    "tuned_feature_importance_df.plot(kind='bar', figsize=(15, 7))\n",
    "plt.title('Tuned Feature Importance across Seasons')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.legend(title='Season')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Seasons for x-axis\n",
    "x = metrics_table['Season']\n",
    "\n",
    "# Plotting each metric as a line for the original model\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    plt.plot(x, metrics_table[metric], marker='o', linestyle='-', label=f'Original {metric}')\n",
    "\n",
    "# Plotting each metric as a line for the tuned model\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    plt.plot(x, tuned_metrics_table[metric], marker='x', linestyle='--', label=f'Tuned {metric}')\n",
    "\n",
    "# Setting the title, labels, and legend\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparison of Original vs. Tuned Performance Metrics Across Seasons')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Tree Visualization (we'll visualize just one tree for simplicity)\n",
    "\n",
    "# Initialize list to store tree depths for each season\n",
    "tree_depths = {season: [] for season in SEASONS}\n",
    "\n",
    "# Extract tree depths for each season\n",
    "for rf, season in zip(rf_models, SEASONS):\n",
    "    depths = [estimator.tree_.max_depth for estimator in rf.estimators_]\n",
    "    tree_depths[season] = depths\n",
    "\n",
    "# Plotting tree depth distribution\n",
    "for season, depths in tree_depths.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(depths, bins=20, edgecolor=\"k\", alpha=0.7)\n",
    "    plt.title(f'Tree Depth Distribution for {season} Season')\n",
    "    plt.xlabel('Tree Depth')\n",
    "    plt.ylabel('Number of Trees')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "features = X_train.columns.tolist()\n",
    "\n",
    "for rf, season in zip(rf_models, SEASONS):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(rf.estimators_[0], filled=True, feature_names=features, class_names=True, max_depth=3)\n",
    "    plt.title(f'One of the Trees from the Random Forest for {season}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. OOB Error Rate For Each Season\n",
    "oob_errors = []\n",
    "\n",
    "# Model Training and OOB computation\n",
    "for season, train_data in zip(SEASONS, train_df):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "\n",
    "    rf = RandomForestClassifier(oob_score=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    oob_error = 1 - rf.oob_score_\n",
    "    oob_errors.append(oob_error)\n",
    "\n",
    "# Displaying the OOB errors for each season\n",
    "oob_error_df = pd.DataFrame({\n",
    "    'Season': SEASONS,\n",
    "    'OOB Error Rate': oob_errors\n",
    "})\n",
    "\n",
    "oob_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Confusion Matrix\n",
    "# Function to plot confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot confusion matrix for each season\n",
    "for rf, eval_data, season in zip(rf_models, eval_df, SEASONS):\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_eval = eval_data[\"labels\"]\n",
    "    y_pred = rf.predict(X_eval)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_eval, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[\"Class 0\", \"Class 1\"], title=f'Confusion Matrix for {season} Season')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Class Prediction Probabilities\n",
    "\n",
    "y_pred_probas = []\n",
    "for rf, eval_data in zip(rf_models, eval_df):\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_pred_proba = rf.predict_proba(X_eval)\n",
    "    y_pred_probas.append(y_pred_proba)\n",
    "\n",
    "# Plot class prediction probabilities for each season\n",
    "for y_pred_proba, season in zip(y_pred_probas, SEASONS):\n",
    "    # Get the number of predicted classes\n",
    "    n_classes = y_pred_proba.shape[1]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(n_classes):\n",
    "        plt.hist(y_pred_proba[:, i], bins=50, alpha=0.5, label=f'Class {i}')\n",
    "    plt.title(f'Class Prediction Probabilities for {season} Season')\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ROC Curve and AUC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Binarily encode the labels for each season\n",
    "y_bin_evals = [label_binarize(y, classes=[0, 1, 2]) for y in [eval_data[\"labels\"] for eval_data in eval_df]]\n",
    "\n",
    "# Plot ROC and compute AUC for each season\n",
    "for y_bin_eval, y_pred_proba, season in zip(y_bin_evals, y_pred_probas, SEASONS):\n",
    "    num_classes = y_pred_proba.shape[1]\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin_eval[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {season} Season')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Precision-Recall Curve\n",
    "for idx, season in enumerate(SEASONS):\n",
    "    X_eval = eval_df[idx].drop(columns=\"labels\")\n",
    "    y_eval = eval_df[idx][\"labels\"]\n",
    "\n",
    "    # Using the previously trained random forest models\n",
    "    rf_model = rf_models[idx]\n",
    "\n",
    "    y_pred_proba = rf_model.predict_proba(X_eval)\n",
    "    unique_classes = np.unique(y_eval)\n",
    "    y_bin_eval = label_binarize(y_eval, classes=unique_classes)\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    \n",
    "    n_classes = y_pred_proba.shape[1]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(n_classes):\n",
    "        if n_classes == 1:\n",
    "            # Handle binary case\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_bin_eval.ravel(), y_pred_proba[:, i])\n",
    "            plt.plot(recall[i], precision[i], lw=2, label=f'Precision-Recall curve of class {unique_classes[i]}')\n",
    "        else:\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_bin_eval[:, i], y_pred_proba[:, i])\n",
    "            plt.plot(recall[i], precision[i], lw=2, label=f'Precision-Recall curve of class {unique_classes[i]}')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve for {season} Season')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Box Plot\n",
    "# Plotting box plots for each season separately\n",
    "for season, season_data in zip(SEASONS, train_df):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    season_data.boxplot()\n",
    "    plt.title(f'Feature Distribution in {season} Season')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHHkJva9YDta"
   },
   "source": [
    "## Justin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_models_joblib(model_list, prefix):\n",
    "    for i, model in enumerate(model_list):\n",
    "        model_filename = f\"{prefix}_{NAME_SEASON[i]}.pkl\"\n",
    "        joblib.dump(model, model_filename)\n",
    "        \n",
    "def load_models_joblib(prefix, num_models=3):\n",
    "    loaded_models = []\n",
    "    for i in range(num_models):\n",
    "        model_filename = f\"{prefix}_{NAME_SEASON[i]}.pkl\"\n",
    "        loaded_model = joblib.load(model_filename)\n",
    "        loaded_models.append(loaded_model)\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "for season, train_data, eval_data in zip(SEASONS, train_df, eval_df):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_eval = eval_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv0aynHpUZsY",
    "tags": []
   },
   "source": [
    "### improved ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9njkDyDDYDta",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def get_ann2_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, input_dim=NUM_INPUTS, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ann2_list = []\n",
    "for i, (train_dataset, eval_dataset) in enumerate(zip(train_dataset_list, eval_dataset_list)):\n",
    "    model_ann2, history_ann2 = train_ann_model(get_ann2_model(), train_dataset, eval_dataset)\n",
    "    model_ann2_list.append(model_ann2)\n",
    "    plot_model_history(history_ann2, NAME_SEASON[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(model_ann_baseline_list):\n",
    "    model.save(f\"{MODEL_SAVE_PATH}model_ann2/{NAME_SEASON[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THPtpfUyUWnX"
   },
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 11, 111],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "best_models_xgb = []\n",
    "\n",
    "\n",
    "season_feature_importances_xgb = {}\n",
    "\n",
    "for i, (train_dataframe, eval_dataframe) in enumerate(zip(train_df, eval_df)):\n",
    "    train_dataframe = pd.get_dummies(train_dataframe, columns=['labels'])\n",
    "    eval_dataframe = pd.get_dummies(eval_dataframe, columns=['labels'])\n",
    "\n",
    "    train_dataframe_X = train_dataframe.loc[:, ~train_dataframe.columns.str.startswith('labels_')]\n",
    "    eval_dataframe_X = eval_dataframe.loc[:, ~eval_dataframe.columns.str.startswith('labels_')]\n",
    "    \n",
    "    train_dataframe_Y = train_dataframe.loc[:, train_dataframe.columns.str.startswith('labels_')]\n",
    "    eval_dataframe_Y = eval_dataframe.loc[:, eval_dataframe.columns.str.startswith('labels_')]\n",
    "\n",
    "    model = xgb.XGBClassifier()\n",
    "    \n",
    "    n_iter_search = 10  \n",
    "    random_search = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_distributions=param_dist, \n",
    "        n_iter=n_iter_search, \n",
    "        cv=3, \n",
    "        n_jobs=-1, \n",
    "        verbose=1, \n",
    "        scoring='accuracy')\n",
    "    random_search.fit(train_dataframe_X, train_dataframe_Y)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    model.fit(train_dataframe_X, train_dataframe_Y)\n",
    "\n",
    "    eval_predictions = model.predict(eval_dataframe_X)\n",
    "    \n",
    "    accuracy = accuracy_score(eval_dataframe_Y, eval_predictions)\n",
    "    recall = recall_score(eval_dataframe_Y, eval_predictions, average='macro')\n",
    "\n",
    "    # The confusion matrix takes a vector of labels (not the one-hot encoding)\n",
    "    conf_matrix = confusion_matrix(eval_dataframe_Y.values.argmax(axis=1), eval_predictions.argmax(axis=1))\n",
    "    feature_importance = model.feature_importances_\n",
    "    \n",
    "    print(f\"Model {i + 1} - Accuracy: {accuracy}\")\n",
    "    print(f\"Model {i + 1} - Recall: {recall}\")\n",
    "    print(f\"Model {i + 1} - Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    season_feature_importances_xgb[f'Model {i + 1}'] = feature_importance\n",
    "\n",
    "    best_models_xgb.append(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(best_models_xgb):\n",
    "    model.save_model(f'xgb_model_{i + 1}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_xgb=[]\n",
    "loaded_model1 = xgb.Booster(model_file='xgb_model_1.json')\n",
    "loaded_model2 = xgb.Booster(model_file='xgb_model_2.json')\n",
    "loaded_model3 = xgb.Booster(model_file='xgb_model_3.json')\n",
    "best_models_xgb.append(loaded_model1)\n",
    "best_models_xgb.append(loaded_model2)\n",
    "best_models_xgb.append(loaded_model3)\n",
    "best_models_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_num, importances in season_feature_importances_xgb.items():\n",
    "    print(f\"Feature Importances for {model_num}\")\n",
    "    for feature, importance in zip(train_dataframe_X.columns, importances):\n",
    "        print(f\"{feature}: {importance}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_df_xgb = pd.DataFrame(season_feature_importances_xgb, index=X_train.columns)\n",
    "\n",
    "for i, (season,train_dataframe, eval_dataframe) in enumerate(zip(SEASONS, train_df, eval_df)):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance_df_xgb[f'Model {i + 1}'].sort_values().plot(kind='barh', title=f'Feature Importance for {season}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "feature_importance_df_xgb.plot(kind='bar', figsize=(15, 7))\n",
    "plt.title('Feature Importance across Seasons')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.legend(title='Season')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season, train_data, eval_data in zip(SEASONS, train_df, eval_df):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_eval = eval_data[\"labels\"]\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "param_dist = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'sigmoid', 'poly'],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto'],\n",
    "    'degree': [2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "best_models_svm = []\n",
    "\n",
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "for i, (train_data, eval_data) in enumerate(zip(train_df, eval_df)):\n",
    "    svm = SVC()\n",
    "    \n",
    "    n_iter_search = 10\n",
    "    random_search = RandomizedSearchCV(svm, param_distributions=param_dist, n_iter=n_iter_search, cv=3, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_svm = random_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_svm.predict(X_eval)\n",
    "    \n",
    "    accuracy = accuracy_score(y_eval, y_pred)\n",
    "    recall = recall_score(y_eval, y_pred, average='macro')\n",
    "    f1 = f1_score(y_eval, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_eval, y_pred)\n",
    "    \n",
    "    print(f\"Model {i + 1} - Accuracy: {accuracy}\")\n",
    "    print(f\"Model {i + 1} - Recall: {recall}\")\n",
    "    print(f\"Model {i + 1} - F1-Score: {f1}\")\n",
    "    print(f\"Model {i + 1} - Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    best_models_svm.append(best_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_models_svm, 'svm_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_loaded_models = joblib.load('svm_models.pkl')\n",
    "svm_loaded_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "num_epochs=10\n",
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "def create_lstm_model(hidden_units=64, lstm_layers=1, dense_units=32, dropout_rate=0.0):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(hidden_units, input_shape=(1, num_features), return_sequences=True))\n",
    "    \n",
    "    for _ in range(lstm_layers - 1):\n",
    "        model.add(keras.layers.LSTM(hidden_units, return_sequences=True))\n",
    "    \n",
    "    model.add(keras.layers.LSTM(hidden_units, return_sequences=False))\n",
    "    model.add(keras.layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_rate))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                  metrics=[keras.metrics.BinaryAccuracy(name='Accuracy'), \n",
    "                           keras.metrics.Recall(name='Recall'),\n",
    "                          ])\n",
    "    return model\n",
    "\n",
    "param_dist = {\n",
    "    'hidden_units': np.arange(32, 128, 32),\n",
    "    'lstm_layers': np.arange(1, 4),\n",
    "    'dense_units': np.arange(16, 64, 16),\n",
    "    'dropout_rate': [0.0, 0.2, 0.4, 0.6, 0.8],\n",
    "}\n",
    "\n",
    "all_metrics_lstm = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1-Score\": [],\n",
    "}\n",
    "\n",
    "lstm_best_models = []\n",
    "\n",
    "for season, train_data, eval_data in zip(SEASONS, train_df, eval_df):\n",
    "    X_train = np.array(train_data.drop(columns=\"labels\"))\n",
    "    y_train = np.array(train_data[\"labels\"])\n",
    "    X_eval = np.array(eval_data.drop(columns=\"labels\"))\n",
    "    y_eval = np.array(eval_data[\"labels\"])\n",
    "\n",
    "    num_time_steps, num_features = X_train.shape\n",
    "    X_train = X_train.reshape(num_time_steps, 1, num_features)\n",
    "    num_time_steps, num_features = X_eval.shape\n",
    "    X_eval = X_eval.reshape(num_time_steps, 1, num_features)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    for _ in range(10): \n",
    "        hidden_units = np.random.choice(param_dist['hidden_units'])\n",
    "        lstm_layers = np.random.choice(param_dist['lstm_layers'])\n",
    "        dense_units = np.random.choice(param_dist['dense_units'])\n",
    "        dropout_rate = np.random.choice(param_dist['dropout_rate'])\n",
    "\n",
    "        model = create_lstm_model(hidden_units, lstm_layers, dense_units, dropout_rate)\n",
    "\n",
    "        history_lstm = model.fit(X_train, y_train, epochs=num_epochs, batch_size=32, verbose=0)\n",
    "        y_pred = model.predict(X_eval)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_eval, y_pred_binary)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "\n",
    "    if best_model is not None:\n",
    "        lstm_best_models.append(best_model)\n",
    "        best_model.save(f\"lstm_{season}\")\n",
    "\n",
    "    y_pred = best_model.predict(X_eval)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_eval, y_pred_binary)\n",
    "    precision = precision_score(y_eval, y_pred_binary, average='macro')\n",
    "    recall = recall_score(y_eval, y_pred_binary, average='macro')\n",
    "    f1 = f1_score(y_eval, y_pred_binary, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_eval, y_pred_binary)\n",
    "\n",
    "    all_metrics_lstm[\"Season\"].append(season)\n",
    "    all_metrics_lstm[\"Accuracy\"].append(accuracy)\n",
    "    all_metrics_lstm[\"Precision\"].append(precision)\n",
    "    all_metrics_lstm[\"Recall\"].append(recall)\n",
    "    all_metrics_lstm[\"F1-Score\"].append(f1)\n",
    "\n",
    "    print(f\"Season {season} - Accuracy: {accuracy}\")\n",
    "    print(f\"Season {season} - Recall: {recall}\")\n",
    "    print(f\"Season {season} - F1-Score: {f1}\")\n",
    "    print(f\"Season {season} - Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model_ann(lstm_best_models, \"model_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_lstm = load_model_ann(\"model_lstm\", SEASONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all_metrics_lstm\n",
    "save_metrics(all_metrics_lstm, \"all_metrics_lstm.pkl\")\n",
    "\n",
    "# Load all_metrics_lstm\n",
    "loaded_all_metrics_lstm = load_metrics(\"all_metrics_lstm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "markers = ['o', 's', '^', 'x']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.bar(all_metrics_lstm[\"Season\"], all_metrics_lstm[metric], color=colors[i], label=metric)\n",
    "    plt.title(f'{metric} for Different Seasons')\n",
    "    plt.xlabel('Seasons')\n",
    "    plt.ylabel(metric)\n",
    "    plt.ylim(0, 1) \n",
    "    plt.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- knn randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "param_dist = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],\n",
    "}\n",
    "\n",
    "all_metrics_knn = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1-Score\": [],\n",
    "}\n",
    "\n",
    "best_knn_models=[]\n",
    "\n",
    "for season, train_data, eval_data in zip(SEASONS, train_df, eval_df):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_eval = eval_data[\"labels\"]\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    random_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42, verbose=1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_knn = random_search.best_estimator_\n",
    "    best_knn_models.append(best_knn)\n",
    "\n",
    "    y_pred = best_knn.predict(X_eval)\n",
    "\n",
    "    accuracy = accuracy_score(y_eval, y_pred)\n",
    "    precision = precision_score(y_eval, y_pred, average='macro')\n",
    "    recall = recall_score(y_eval, y_pred, average='macro')\n",
    "    f1 = f1_score(y_eval, y_pred, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_eval, y_pred)\n",
    "\n",
    "    all_metrics_knn[\"Season\"].append(season)\n",
    "    all_metrics_knn[\"Accuracy\"].append(accuracy)\n",
    "    all_metrics_knn[\"Precision\"].append(precision)\n",
    "    all_metrics_knn[\"Recall\"].append(recall)\n",
    "    all_metrics_knn[\"F1-Score\"].append(f1)\n",
    "\n",
    "    print(f\"Season {season} - Accuracy: {accuracy}\")\n",
    "    print(f\"Season {season} - Recall: {recall}\")\n",
    "    print(f\"Season {season} - F1-Score: {f1}\")\n",
    "    print(f\"Season {season} - Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_joblib(best_knn_models,\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not best_knn_models:\n",
    "    best_knn_models = load_models_joblib(\"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tabformer random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "best_models_tabnet = []\n",
    "\n",
    "SEASONS = [\"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "param_dist = {\n",
    "    'n_d': [8, 16, 32, 64],\n",
    "    'n_a': [8, 16, 32, 64],\n",
    "    'n_steps': [3, 4, 5],\n",
    "    'gamma': [1.0, 1.3, 1.5],\n",
    "    'n_independent': [1, 2, 3],\n",
    "}\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score)}\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (train_data, eval_data) in enumerate(zip(train_df, eval_df)):\n",
    "    X_train = train_data.drop(columns=\"labels\")\n",
    "    y_train = train_data[\"labels\"]\n",
    "    X_eval = eval_data.drop(columns=\"labels\")\n",
    "    y_eval = eval_data[\"labels\"]\n",
    "\n",
    "    clf = TabNetClassifier()\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        clf,\n",
    "        param_distributions=param_dist,\n",
    "        scoring=scoring,\n",
    "        refit='accuracy',\n",
    "        n_iter=7, \n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        verbose=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train.to_numpy(), y_train.to_numpy(),\n",
    "                eval_set=[(X_eval.to_numpy(), y_eval.to_numpy())],eval_metric=['accuracy'],patience=20)\n",
    "\n",
    "\n",
    "    best_tabnet = random_search.best_estimator_\n",
    "\n",
    "    y_pred = best_tabnet.predict(X_eval.to_numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_eval, y_pred)\n",
    "    recall = recall_score(y_eval, y_pred, average='macro')\n",
    "    f1 = f1_score(y_eval, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_eval, y_pred)\n",
    "\n",
    "    print(f\"Model {i + 1} - Best Parameters: {random_search.best_params_}\")\n",
    "    print(f\"Model {i + 1} - Accuracy: {accuracy}\")\n",
    "    print(f\"Model {i + 1} - Recall: {recall}\")\n",
    "    best_models_tabnet.append(best_tabnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_joblib(best_models_tabnet, \"tabnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models_tabnet = load_models_joblib(\"tabnet\")\n",
    "loaded_models_tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize and prepare data\n",
    "def tokenize_data(texts, labels, tokenizer, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    for text in texts:\n",
    "        encoding = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = tokenize_data(X_train, y_train, tokenizer)\n",
    "eval_dataset = tokenize_data(X_eval, y_eval, tokenizer)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# Set up training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    y_pred.extend(logits.argmax(dim=1).tolist())\n",
    "    y_true.extend(labels.tolist())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwoPebeVhsuG"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepraing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset_input_list = []\n",
    "test_dataset_label_list = []\n",
    "for season_name in NAME_SEASON:\n",
    "    dataset = get_dataset(f'{TEST_OUTPUT_PATH}{season_name}*')\n",
    "    # dataset.map(lambda input,label: tf.expand_dims(input, axis=0),label)\n",
    "    # test_dataset_input_list.append(dataset)    \n",
    "    dataset_input = dataset.map(lambda input,_: tf.expand_dims(input, axis=0))\n",
    "    # pprint(dataset_input)\n",
    "    dataset_label = dataset.map(lambda _,label: label)\n",
    "    test_dataset_input_list.append(dataset_input)    \n",
    "    test_dataset_label_list.append(dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_spring = get_dataset(f'{TEST_OUTPUT_PATH}Spring*')\n",
    "test_dataset_summer = get_dataset(f'{TEST_OUTPUT_PATH}Summer*')\n",
    "test_dataset_fall = get_dataset(f'{TEST_OUTPUT_PATH}Fall*')\n",
    "# test_balance_dataset(test_dataset_spring)\n",
    "# test_balance_dataset(test_dataset_summer)\n",
    "# test_balance_dataset(test_dataset_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dataset(dataset: tf.data.Dataset, batch_size) -> tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "    indexed_dataset = dataset.enumerate()  # add an index to each example\n",
    "    test_dataset = (\n",
    "        indexed_dataset.filter(lambda i, _: i % 100 <= 100)\n",
    "        .map(lambda _, data: data)  \n",
    "        .batch(batch_size) \n",
    "    )\n",
    "    return (test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_all_seasons = [test_dataset_spring, test_dataset_summer, test_dataset_fall]\n",
    "test_dataset_list = []\n",
    "\n",
    "for dataset in test_dataset_all_seasons:\n",
    "    test_season = batch_dataset(dataset, batch_size=32)\n",
    "    test_dataset_list.append(test_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = [convert_dataset_to_dataframe(dataset) for dataset in test_dataset_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[1]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_path(model_name):\n",
    "    model_list = []\n",
    "    for season_name in NAME_SEASON:\n",
    "        model = tf.keras.models.load_model(f\"{MODEL_SAVE_PATH}{model_name}/{season_name}\")\n",
    "        model_list.append(model)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predict_list(model_list, test_dataset_input_list):\n",
    "    predict_label_list = []\n",
    "    for season, model, inputs in zip(NAME_SEASON, model_list, test_dataset_input_list):\n",
    "        predict_label = model.predict(inputs)\n",
    "        predict_label_list.append(predict_label)\n",
    "    return predict_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "import tensorflow.keras.metrics as metrics\n",
    "\n",
    "def plot_confusion_matrix(test_dataset_label_list, predict_label_list_ann):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for each pair of real and predicted labels in the given datasets.\n",
    "\n",
    "    Args:\n",
    "    test_dataset_label_list (list of tf.Tensor): A list of TensorFlow tensors containing real labels.\n",
    "    predict_label_list_ann (list of tf.Tensor): A list of TensorFlow tensors containing predicted labels.\n",
    "\n",
    "    \"\"\"\n",
    "    metric_list = [metrics.Recall(name = 'Recall'), metrics.BinaryAccuracy(name = 'Accuracy')]\n",
    "    for real_labels, predict_labels in zip(test_dataset_label_list, predict_label_list_ann):\n",
    "        predicted = np.array([1 if x >= 0.5 else 0 for x in predict_labels])\n",
    "        actual = np.array([x for x in real_labels])\n",
    "        \n",
    "        for m in metric_list:\n",
    "            m.update_state(actual, predicted)\n",
    "            print(f'{m.name}: {m.result()}')\n",
    "        \n",
    "        conf_mat = confusion_matrix(actual, predicted)\n",
    "        displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        displ.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mia - ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ann_baseline_list = get_model_from_path('model_ann_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_label_list_ann_baseline = generate_predict_list(model_ann_baseline_list, test_dataset_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_dataset_label_list, predict_label_list_ann_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN deeper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ann_deep_list = get_model_from_path('model_ann_deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_label_list_ann_deep = generate_predict_list(model_ann_deep_list, test_dataset_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_dataset_label_list, predict_label_list_ann_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_test_input = np.asarray(list(test_dataset_input_list[0]))\n",
    "np_test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, test_input, test_label, model_name, season_name, convert_back_to_tf_dataset: bool):\n",
    "    results = []\n",
    "    if convert_back_to_tf_dataset:\n",
    "        pred = model.predict(tf.data.Dataset.from_tensor_slices(test_input), verbose=0).squeeze()\n",
    "    else:\n",
    "        pred = model.predict(test_input, verbose=0).squeeze()\n",
    "        \n",
    "    baseline_mae = np.mean(np.abs(pred-test_label ))\n",
    "    results.append({'feature':'BASELINE','mae':baseline_mae})      \n",
    "    \n",
    "    for i, band in enumerate(BANDS):\n",
    "        tmp_test_input = test_input[:,:,i].copy()\n",
    "        np.random.shuffle(test_input[:,:,i])\n",
    "        if convert_back_to_tf_dataset:\n",
    "            pred = model.predict(tf.data.Dataset.from_tensor_slices(test_input), verbose=0).squeeze()\n",
    "        else:\n",
    "            pred = model.predict(test_input, verbose=0).squeeze()\n",
    "                \n",
    "        mae = np.mean(np.abs(pred - test_label))\n",
    "        results.append({'feature':band,'mae':mae})\n",
    "        print(f'{band}: {mae}')\n",
    "        test_input[:,:,i] = tmp_test_input\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values('mae')\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.barh(np.arange(len(BANDS)+1),df.mae)\n",
    "    plt.yticks(np.arange(len(BANDS)+1),df.feature.values)\n",
    "    plt.title(f'{model_name} {season_name} Feature Importance',size=16)\n",
    "    plt.ylim((-1,len(BANDS)+1))\n",
    "    plt.plot([baseline_mae,baseline_mae],[-1,len(BANDS)+1], '--', color='orange',\n",
    "             label=f'Baseline OOF\\nMAE={baseline_mae:.3f}')\n",
    "    plt.xlabel(f'OOF MAE with feature permuted',size=14)\n",
    "    plt.ylabel('Feature',size=14)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ann_feature_importance(model_list, model_name, convert_back_to_tf_dataset: bool):\n",
    "    for i, model in enumerate(model_list):\n",
    "        analyze_feature_importance(model, \n",
    "                                   np.asarray(list(test_dataset_input_list[i])), \n",
    "                                   np.asarray(list(test_dataset_label_list[i])), model_name, NAME_SEASON[i], convert_back_to_tf_dataset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyze_ann_feature_importance(model_ann_baseline_list, 'ANN Baseline', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyze_ann_feature_importance(model_ann_deep_list, 'ANN Deep', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huizi - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_per_season = {}\n",
    "\n",
    "for season, data in zip(SEASONS, test_df):\n",
    "    X_test = data.drop(columns=\"labels\")\n",
    "    y_test = data[\"labels\"]\n",
    "\n",
    "    rf = RandomForestClassifier(**best_params_per_season[season])\n",
    "    rf.fit(X_train, y_train)  # Assuming the training data remains the same\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    predicted_labels_per_season[season] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_test = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Recall\": []\n",
    "}\n",
    "\n",
    "for season, data in zip(SEASONS, test_df):\n",
    "    y_test = data[\"labels\"]\n",
    "    y_pred = predicted_labels_per_season[season]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    all_metrics_test[\"Season\"].append(season)\n",
    "    all_metrics_test[\"Accuracy\"].append(accuracy)\n",
    "    all_metrics_test[\"Recall\"].append(recall)\n",
    "\n",
    "metrics_test_table = pd.DataFrame(all_metrics_test)\n",
    "print(metrics_test_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season, data in zip(SEASONS, test_df):\n",
    "    y_test = data[\"labels\"]\n",
    "    y_pred = predicted_labels_per_season[season]\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "    \n",
    "    plt.figure()\n",
    "    displ.plot()\n",
    "    plt.title(f'Confusion Matrix for {season}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict using the default parameters of RandomForest\n",
    "predicted_labels_default_per_season = {}\n",
    "\n",
    "for season, data in zip(SEASONS, test_df):\n",
    "    X_test = data.drop(columns=\"labels\")\n",
    "    y_test = data[\"labels\"]\n",
    "\n",
    "    # Using default RandomForestClassifier\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)  # Assuming the training data remains the same\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    predicted_labels_default_per_season[season] = y_pred\n",
    "\n",
    "# Compute metrics for predictions with default parameters\n",
    "all_metrics_test_default = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Recall\": []\n",
    "}\n",
    "\n",
    "for season, data in zip(SEASONS, test_df):\n",
    "    y_test = data[\"labels\"]\n",
    "    y_pred = predicted_labels_default_per_season[season]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    all_metrics_test_default[\"Season\"].append(season)\n",
    "    all_metrics_test_default[\"Accuracy\"].append(accuracy)\n",
    "    all_metrics_test_default[\"Recall\"].append(recall)\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "metrics_test_default_table = pd.DataFrame(all_metrics_test_default)\n",
    "print(metrics_test_default_table)\n",
    "\n",
    "# Display confusion matrices for predictions with default parameters\n",
    "for season, data in zip(SEASONS, test_df):\n",
    "    y_test = data[\"labels\"]\n",
    "    y_pred = predicted_labels_default_per_season[season]\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "    \n",
    "    plt.figure()\n",
    "    displ.plot()\n",
    "    plt.title(f'Confusion Matrix (Default Params) for {season}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann2_list = []\n",
    "for season_name in NAME_SEASON:\n",
    "    model = tf.keras.models.load_model(f\"{MODEL_SAVE_PATH}model_ann2/{season_name}\")\n",
    "    pprint(model)\n",
    "    model_ann2_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label_list_ann2 = generate_predict_list(model_ann2_list, test_dataset_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_dataset_label_list, predict_label_list_ann2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_season_feature_importances_xgb = {}\n",
    "\n",
    "for i, (test_dataframe_season, model) in enumerate(zip(test_df, best_models_xgb)):\n",
    "    test_dataframe_season = pd.get_dummies(test_dataframe_season, columns=['labels'])\n",
    "    \n",
    "    test_dataframe_X = test_dataframe_season.loc[:, ~test_dataframe_season.columns.str.startswith('labels_')]\n",
    "    test_dataframe_Y = test_dataframe_season.loc[:, test_dataframe_season.columns.str.startswith('labels_')]\n",
    "    \n",
    "    test_prediction = model.predict(xgb.DMatrix(test_dataframe_X), output_margin=True)\n",
    "    # Use output_margin=True to obtain probability scores instead of class labels\n",
    "    \n",
    "    accuracy = accuracy_score(test_dataframe_Y, (test_prediction > 0.5).astype(int))\n",
    "    recall = recall_score(test_dataframe_Y, (test_prediction > 0.5).astype(int), average='macro')\n",
    "    \n",
    "    conf_matrix = confusion_matrix(test_dataframe_Y.values.argmax(axis=1), (test_prediction > 0.5).astype(int).argmax(axis=1))\n",
    "    # feature_importance = model.feature_importances_\n",
    "    \n",
    "    print(f\"Model {i + 1} - Accuracy: {accuracy}\")\n",
    "    print(f\"Model {i + 1} - Recall: {recall}\")\n",
    "    print(f\"Model {i + 1} - Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    # test_season_feature_importances_xgb[f'Model {i + 1}'] = feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "predicted_labels_per_season = {}\n",
    "\n",
    "# Define data for test seasons\n",
    "test_seasons = test_df  # Assuming you have test data for each season in the test_df list\n",
    "\n",
    "all_metrics_test = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Recall\": [],\n",
    "    \"Confusion Matrix\": []\n",
    "}\n",
    "\n",
    "for season, data in zip(SEASONS, test_seasons):\n",
    "    X_test = data.drop(columns=\"labels\").to_numpy()\n",
    "    y_test = data[\"labels\"].to_numpy()\n",
    "\n",
    "    knn_model = best_knn_models[SEASONS.index(season)]\n",
    "\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "    predicted_labels_per_season[season] = y_pred\n",
    "\n",
    "for season, data in zip(SEASONS, test_seasons):\n",
    "    y_test = data[\"labels\"].to_numpy()\n",
    "    y_pred = predicted_labels_per_season[season]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    all_metrics_test[\"Season\"].append(season)\n",
    "    all_metrics_test[\"Accuracy\"].append(accuracy)\n",
    "    all_metrics_test[\"Recall\"].append(recall)\n",
    "    all_metrics_test[\"Confusion Matrix\"].append(confusion_matrix_result)\n",
    "\n",
    "metrics_test_table = pd.DataFrame(all_metrics_test)\n",
    "pprint(metrics_test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not best_models_tabnet:\n",
    "    best_models_tabnet = load_models_joblib(\"tabnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted_labels_per_season = {}\n",
    "\n",
    "# Define data for test seasons\n",
    "test_seasons = test_df  # Assuming you have test data for each season in the test_df list\n",
    "\n",
    "all_metrics_test = {\n",
    "    \"Season\": [],\n",
    "    \"Accuracy\": [],\n",
    "    \"Recall\": [],\n",
    "    \"Confusion Matrix\": []\n",
    "}\n",
    "\n",
    "for season, data in zip(SEASONS, test_seasons):\n",
    "    X_test = data.drop(columns=\"labels\").to_numpy()\n",
    "    y_test = data[\"labels\"].to_numpy()\n",
    "\n",
    "    # Retrieve the best TabNet model for the current season\n",
    "    tabnet_model = best_models_tabnet[SEASONS.index(season)]\n",
    "\n",
    "    y_pred = tabnet_model.predict(X_test)\n",
    "    predicted_labels_per_season[season] = y_pred\n",
    "\n",
    "for season, data in zip(SEASONS, test_seasons):\n",
    "    y_test = data[\"labels\"].to_numpy()\n",
    "    y_pred = predicted_labels_per_season[season]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    all_metrics_test[\"Season\"].append(season)\n",
    "    all_metrics_test[\"Accuracy\"].append(accuracy)\n",
    "    all_metrics_test[\"Recall\"].append(recall)\n",
    "    all_metrics_test[\"Confusion Matrix\"].append(confusion_matrix_result)\n",
    "\n",
    "metrics_test_table = pd.DataFrame(all_metrics_test)\n",
    "pprint(metrics_test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mia - Fine tuning Deeper ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras_tuner \n",
    "\n",
    "\n",
    "def call_existing_code(units_0, layer_num, activation, dropout, dropout_rate, lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=units_0, input_dim=NUM_INPUTS, activation='relu', name='Dense0'))\n",
    "    cur_layer = 1\n",
    "    while(cur_layer < layer_num):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f\"units_{cur_layer}\", min_value=32, max_value=512, step=32), \n",
    "            activation='relu', \n",
    "            name=f'Dense{cur_layer}'))\n",
    "        if dropout:\n",
    "            model.add(layers.Dropout(rate=dropout_rate))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        cur_layer += 1\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid', name='Output'))\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr), \n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=all_metric_names[0]), \n",
    "            tf.keras.metrics.Recall(name=all_metric_names[1]),\n",
    "            tf.keras.metrics.AUC(name=all_metric_names[2]),\n",
    "            tf.keras.metrics.FalseNegatives(name=all_metric_names[3])], \n",
    "        loss ='binary_crossentropy',\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    layer_num = hp.Int(\"layer_num\", min_value=1, max_value=6, step=1)\n",
    "    units_0 = hp.Int(\"units_0\", min_value=32, max_value=512, step=32)\n",
    "    activation = hp.Choice(\"activation\", [\"relu\", \"elu\"])\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    \n",
    "    model = call_existing_code(\n",
    "        units_0=units_0, layer_num=layer_num, activation=activation, dropout=dropout, dropout_rate=dropout_rate, lr=lr\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_search_instance(objective, season_name, overwrite=False):\n",
    "    metric_name = ''\n",
    "    if isinstance(objective, str):\n",
    "        metric_name = objective\n",
    "    else:\n",
    "        for i, objective_tune in enumerate(objective):\n",
    "            if i != 0:\n",
    "                metric_name += '_'\n",
    "            metric_name += objective_tune.name\n",
    "    return keras_tuner.RandomSearch(\n",
    "        hypermodel=build_model,\n",
    "        objective=objective,\n",
    "        max_trials=10,\n",
    "        overwrite=overwrite,\n",
    "        directory=\"./model/finetune\",\n",
    "        project_name=f\"finetune_{metric_name}_{season_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_fine_tune(objective, epochs=10):\n",
    "    for i, (train_dataset, eval_dataset) in enumerate(zip(train_dataset_list, eval_dataset_list)):\n",
    "        tf.keras.backend.clear_session()\n",
    "        tuner = get_random_search_instance(objective, NAME_SEASON[i], True)\n",
    "        tuner.search(train_dataset, validation_data=eval_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_best_model(objective):\n",
    "    model_ann_finetuned_list = []\n",
    "    for i, (train_dataset, eval_dataset) in enumerate(zip(train_dataset_list, eval_dataset_list)):\n",
    "        tf.keras.backend.clear_session()\n",
    "        tuner = get_random_search_instance(objective, NAME_SEASON[i], False)\n",
    "        models = tuner.get_best_models()\n",
    "        best_model = models[0]\n",
    "        best_model.build()\n",
    "        best_model.summary()\n",
    "        model_ann_finetuned_list.append(best_model)\n",
    "    return model_ann_finetuned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_with_best_model(model_finetuned_list):\n",
    "    predict_label_list = generate_predict_list(model_finetuned_list, test_dataset_input_list)\n",
    "    plot_confusion_matrix(test_dataset_label_list, predict_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine tune with objective as 'val_Recall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_objective = \"val_Recall\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_fine_tune(finetune_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load best results and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_recall_model_list = load_best_model(finetune_objective)\n",
    "predict_with_best_model(finetuned_recall_model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine tune with objective as 'val_Accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_objective = \"val_Accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_fine_tune(finetune_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load best results and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_recall_model_list = load_best_model(finetune_objective)\n",
    "predict_with_best_model(finetuned_recall_model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine tune with both 'val_Accuracy', 'val_Recall' and 'val_AUC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_objective = [\n",
    "    keras_tuner.Objective('val_Accuracy', \"max\"), \n",
    "    keras_tuner.Objective('val_Recall', \"max\"), \n",
    "    keras_tuner.Objective('val_AUC', \"max\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_fine_tune(finetune_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load best results and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_recall_model_list = load_best_model(finetune_objective)\n",
    "predict_with_best_model(finetuned_recall_model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Mia - FTTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from fttransformer.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from fttransformer.models.fttransformer import FTTransformerEncoder, FTTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfoOnAvpYDtV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_INPUTS = len(feature_list)\n",
    "LABEL = fire_class_band\n",
    "BANDS = []\n",
    "for feature in feature_list:\n",
    "  BANDS += [feature['band']]\n",
    "FEATURE_NAMES = BANDS + [LABEL]\n",
    "\n",
    "NUMERIC_FEATURES = []\n",
    "CATEGORICAL_FEATURES = []\n",
    "\n",
    "def separate_feature():\n",
    "    for band in BANDS:\n",
    "        if band == 'LC_Type1':\n",
    "            CATEGORICAL_FEATURES.append(band)\n",
    "        else:\n",
    "            NUMERIC_FEATURES.append(band)\n",
    "separate_feature()\n",
    "print(NUMERIC_FEATURES)\n",
    "\n",
    "columns = [tf.io.FixedLenFeature(shape=[], dtype=tf.float32) for k in FEATURE_NAMES]\n",
    "FEATURES_DICT = dict(zip(FEATURE_NAMES, columns))\n",
    "print(FEATURES_DICT)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CSV_EXPORT_FOLDER = 'processed_datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLPUBstCZAYx"
   },
   "source": [
    "## Parse tfrecord and export preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "a5kwqeres-X_",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "df96ef7a-cbb3-466d-828f-54862d8600dd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "    \"\"\"The parsing function.\n",
    "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "    Args:\n",
    "    example_proto: a serialized Example.\n",
    "    Returns:\n",
    "    A dictionary of tensors, keyed by feature name.\n",
    "    \"\"\"\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "    return parsed_features\n",
    "\n",
    "def get_dataset_df(pattern):\n",
    "    \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
    "    Get all the files matching the pattern, parse and convert to tuple.\n",
    "    Args:\n",
    "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
    "    Returns:\n",
    "    A tf.data.Dataset\n",
    "    \"\"\"\n",
    "    print(pattern)\n",
    "    glob = tf.io.gfile.glob(pattern)\n",
    "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "    return tfds.as_dataframe(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5I3MzlHSgNay",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_spring = get_dataset_df(f'{TRAINING_OUTPUT_PATH}Spring*')\n",
    "train_dataset_summer = get_dataset_df(f'{TRAINING_OUTPUT_PATH}Summer*')\n",
    "train_dataset_fall = get_dataset_df(f'{TRAINING_OUTPUT_PATH}Fall*')\n",
    "print(train_dataset_spring.head())\n",
    "train_dataset_all_seasons = [train_dataset_spring, train_dataset_summer, train_dataset_fall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (MinMaxScaler, RobustScaler)\n",
    "\n",
    "def get_normalize_features(numerical_feature_list, fit_target, transform_list):\n",
    "    norm = RobustScaler().fit(fit_target[numerical_feature_list])\n",
    "    norm_list=[]\n",
    "    for transform_target in transform_list:\n",
    "        transformed = transform_target.copy()\n",
    "        transformed[numerical_feature_list] = norm.transform(transform_target[numerical_feature_list])\n",
    "        norm_list.append(transformed)\n",
    "    \n",
    "    return norm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for season, train_set, test_set in zip(NAME_SEASON, train_dataset_all_seasons, test_dataset_all_seasons):\n",
    "    [train_norm, test_norm] = get_normalize_features(NUMERIC_FEATURES, train_set, [train_set, test_set])\n",
    "    print(train_norm.head())\n",
    "    print(test_norm.head())\n",
    "    test_norm.to_csv(f'{CSV_EXPORT_FOLDER}{season}_test_set.csv', index=False)    \n",
    "    train_norm.to_csv(f'{CSV_EXPORT_FOLDER}{season}_train_set.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read normalized dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_all_season = []\n",
    "val_all_season = []\n",
    "test_all_season = []\n",
    "\n",
    "for season in NAME_SEASON:\n",
    "    df = pd.read_csv(f'{CSV_EXPORT_FOLDER}{season}_train_set.csv')\n",
    "    df[CATEGORICAL_FEATURES] = df[CATEGORICAL_FEATURES].astype(str)\n",
    "    data_tr, data_val = train_test_split(df, test_size=0.2)\n",
    "    train_all_season.append(data_tr)\n",
    "    val_all_season.append(data_val)\n",
    "    df = pd.read_csv(f'{CSV_EXPORT_FOLDER}{season}_test_set.csv')\n",
    "    df[CATEGORICAL_FEATURES] = df[CATEGORICAL_FEATURES].astype(str)\n",
    "    test_all_season.append(df)\n",
    "    print(f'{season}: \\\n",
    "          {len(train_all_season[len(train_all_season)-1])},\\\n",
    "          {len(val_all_season[len(val_all_season)-1])}, \\\n",
    "          {len(test_all_season[len(test_all_season)-1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batched_data(df_tr, df_val, batch_size=256):\n",
    "    tfds_train = df_to_dataset(df_tr[FEATURE_NAMES], LABEL, shuffle=True, batch_size=batch_size)\n",
    "    tfds_val = df_to_dataset(df_val[FEATURE_NAMES], LABEL, shuffle=False, batch_size=batch_size)\n",
    "    return tfds_train, tfds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fttransformer.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "def call_existing_code(\n",
    "    df_train,\n",
    "    d_embedding:int, \n",
    "    n_layers:int, \n",
    "    ffn_factor:float, \n",
    "    attention_dropout:float, \n",
    "    ffn_dropout:float, \n",
    "    residual_dropout:float, \n",
    "    weight_decay:float, \n",
    "    lr:float):\n",
    "    \n",
    "    ft_linear_encoder = FTTransformerEncoder(\n",
    "        numerical_features=NUMERIC_FEATURES,  # list of numeric features\n",
    "        categorical_features=CATEGORICAL_FEATURES,  # list of numeric features\n",
    "        numerical_data=df_train[NUMERIC_FEATURES].values,\n",
    "        categorical_data=df_train[CATEGORICAL_FEATURES].values,\n",
    "        y = None,\n",
    "        numerical_embedding_type='linear',\n",
    "        embedding_dim=d_embedding,\n",
    "        depth=n_layers,\n",
    "        ffn_factor=ffn_factor,\n",
    "        attn_dropout=attention_dropout,\n",
    "        ff_dropout=ffn_dropout,\n",
    "        residual_dropout=residual_dropout,\n",
    "        explainable=True\n",
    "    )\n",
    "\n",
    "    ft_model = FTTransformer(\n",
    "        encoder=ft_linear_encoder,  # Encoder from above\n",
    "        out_dim=1,  # Number of outputs in final layer\n",
    "        out_activation='sigmoid',  # Activation function for final layer\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    ft_model.compile(\n",
    "        optimizer = optimizer,\n",
    "        # metrics=[\n",
    "        #     # tf.keras.metrics.BinaryAccuracy(name=all_metric_names[0]), \n",
    "        #     tf.keras.metrics.Recall(name=all_metric_names[1]),\n",
    "        #     tf.keras.metrics.AUC(name=all_metric_names[2]),\n",
    "        #     # tf.keras.metrics.FalseNegatives(name=all_metric_names[3])\n",
    "        # ], \n",
    "        metrics= {\"output\": [\n",
    "            tf.keras.metrics.BinaryAccuracy(name=all_metric_names[0]),\n",
    "            tf.keras.metrics.Recall(name=all_metric_names[1]),\n",
    "            tf.keras.metrics.AUC(name=all_metric_names[2], curve='PR'),\n",
    "            tf.keras.metrics.FalseNegatives(name=all_metric_names[3])\n",
    "        ], \"importances\": None},\n",
    "        loss = {\"output\": tf.keras.losses.BinaryCrossentropy(), \"importances\": None},\n",
    "    )\n",
    "    \n",
    "    return ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_default_ft(df_train, data_train, data_val):\n",
    "    lr = 0.001\n",
    "    weight_decay = 0.0001\n",
    "    ft_model = call_existing_code(\n",
    "        df_train,\n",
    "        d_embedding=32, \n",
    "        n_layers=4, \n",
    "        ffn_factor=1,\n",
    "        attention_dropout=0.1,\n",
    "        ffn_dropout=0.1,\n",
    "        residual_dropout=0.0,\n",
    "        weight_decay=weight_decay,\n",
    "        lr=lr\n",
    "    )\n",
    "    epochs = 200\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_output_Accuracy\", mode=\"max\", patience=20, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f'{MODEL_SAVE_PATH}{MODEL_NAME}',\n",
    "            save_weights_only=True,\n",
    "            save_freq='epoch')\n",
    "    ]\n",
    "    history = ft_model.fit(\n",
    "        data_train, \n",
    "        epochs=epochs, \n",
    "        validation_data=data_val,\n",
    "        callbacks=[callbacks]\n",
    "    )\n",
    "    return ft_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_metric_names = ['Accuracy', 'Recall', 'AUC', 'FalseNegatives']\n",
    "prefix = 'output_'\n",
    "def plot_metrics(history, metric_name, plot_title):\n",
    "    if f'{prefix}{metric_name}' not in history.history:\n",
    "        print()\n",
    "        return\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history.history[f'{prefix}{metric_name}'], label=f'training {metric_name}')\n",
    "    plt.plot(history.history[f'val_{prefix}{metric_name}'], label=f'validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{metric_name}')\n",
    "    plt.title(f'Training and Validation {plot_title} {metric_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_history(history, plot_title):\n",
    "    for metric_name in all_metric_names:\n",
    "        plot_metrics(history, metric_name, plot_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'model/'\n",
    "MODEL_NAME = 'ftmodel_baseline'\n",
    "ftmodel_baseline_list = []\n",
    "for season, train_df, val_df in zip(NAME_SEASON, train_all_season, val_all_season):\n",
    "    tf.keras.backend.clear_session()    \n",
    "    print(train_df.shape)\n",
    "    print(val_df.shape)\n",
    "    tfds_train, tfds_val = get_batched_data(train_df, val_df)\n",
    "    model, history = call_default_ft(train_df, tfds_train, tfds_val)\n",
    "    ftmodel_baseline_list.append(model)\n",
    "    plot_model_history(history, season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model(ftmodel_baseline_list, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset_input_list = []\n",
    "test_dataset_label_list = []\n",
    "for dataset in test_all_season:\n",
    "    dataset_tf = df_to_dataset(dataset[BANDS], shuffle=False, batch_size=1)\n",
    "    test_dataset_input_list.append(dataset_tf)\n",
    "    test_dataset_label_list.append(dataset[[LABEL]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ftmodel_baseline'\n",
    "model_fttransformer_baseline_list = get_model_from_path(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_ftbaseline_list = generate_predict_list(model_fttransformer_baseline_list, test_dataset_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, recall_score\n",
    "from fttransformer.utils.helper import get_model_importances\n",
    "\n",
    "def evaluate_result(actual_label_list, predict_label_list):\n",
    "    for season, actual, prediction in zip(NAME_SEASON, actual_label_list, predict_label_list):\n",
    "        print(season)\n",
    "        print(\"Test Accuracy:\", np.round(accuracy_score(actual, prediction['output'].ravel()>0.5), 4))\n",
    "        print(\"Test Recall:\", np.round(recall_score(actual, prediction['output'].ravel()>0.5), 4))\n",
    "        print(\"Test ROC AUC:\", np.round(roc_auc_score(actual, prediction['output'].ravel()), 4))\n",
    "        print(\"Test PR AUC:\", np.round(average_precision_score(actual, prediction['output'].ravel()), 4))\n",
    "        \n",
    "def plot_feature_importance(actual_label_list, predict_label_list):\n",
    "    for season, actual, prediction in zip(NAME_SEASON, actual_label_list, predict_label_list):\n",
    "        linear_importances = prediction['importances']\n",
    "        linear_importances_df = pd.DataFrame(linear_importances[:, :-1], columns = BANDS)\n",
    "        linear_total_importances = get_model_importances(\n",
    "            linear_importances_df, title=f\"{season} Importances for FT-Transformer with Linear Numerical Embedddings\"\n",
    "        )\n",
    "        print(f\"{season}\")        \n",
    "        print(linear_total_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_result(test_dataset_label_list, predict_ftbaseline_list)\n",
    "plot_feature_importance(test_dataset_label_list, predict_ftbaseline_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune FTTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras_tuner \n",
    "\n",
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def __init__(self, df_train):\n",
    "        self.df_train = df_train\n",
    "        \n",
    "    def build(self, hp):\n",
    "        d_embedding = hp.Int(\"d_embedding\", min_value=32, max_value=512, step=8)\n",
    "        n_layers = hp.Int(\"n_layers\", min_value=1, max_value=4)\n",
    "        ffn_factor = hp.Float(\"ffn_factor\", min_value=2/3, max_value=8/3, sampling=\"linear\")\n",
    "        attention_dropout = hp.Float(\"attention_dropout\", min_value=0.0, max_value=0.5, sampling=\"linear\")\n",
    "        ffn_dropout = hp.Float(\"ffn_dropout\", min_value=0.0, max_value=0.5, sampling=\"linear\")\n",
    "        residual_dropout = hp.Float(\"residual_dropout\", min_value=0.0, max_value=0.2, default=0.0, sampling=\"linear\")\n",
    "\n",
    "        lr = hp.Float(\"lr\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "        weight_decay = hp.Float(\"weight_decay\", min_value=1e-6, max_value=1e-3, sampling=\"log\")\n",
    "\n",
    "        model = call_existing_code(\n",
    "            self.df_train,\n",
    "            d_embedding=d_embedding, \n",
    "            n_layers=n_layers, \n",
    "            ffn_factor=ffn_factor,\n",
    "            attention_dropout=attention_dropout,\n",
    "            ffn_dropout=ffn_dropout,\n",
    "            residual_dropout=residual_dropout,\n",
    "            weight_decay=weight_decay,\n",
    "            lr=lr\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_search_instance(objective, hp, df_train, season_name, overwrite=False):\n",
    "    metric_name = ''\n",
    "    if isinstance(objective, str):\n",
    "        metric_name = objective\n",
    "    else:\n",
    "        for i, objective_tune in enumerate(objective):\n",
    "            if i != 0:\n",
    "                metric_name += '_'\n",
    "            metric_name += objective_tune.name\n",
    "    m = MyHyperModel(df_train)\n",
    "    return keras_tuner.RandomSearch(\n",
    "        hypermodel=m,\n",
    "        objective=objective,\n",
    "        max_trials=30,\n",
    "        overwrite=overwrite,\n",
    "        directory=\"./model/finetune-fttransformer\",\n",
    "        project_name=f\"finetune_{metric_name}_{season_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_fine_tune(objective, epochs=10):\n",
    "    for season, train_df, val_df in zip(NAME_SEASON, train_all_season, val_all_season):\n",
    "        tf.keras.backend.clear_session()    \n",
    "        hp = keras_tuner.HyperParameters()        \n",
    "        tfds_train, tfds_val = get_batched_data(train_df, val_df)\n",
    "        tuner = get_random_search_instance(objective, hp, train_df, season, True)\n",
    "        tuner.search(tfds_train, validation_data=tfds_val, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_best_model(objective):\n",
    "    best_models = []\n",
    "    for season, train_df, val_df in zip(NAME_SEASON, train_all_season, val_all_season):\n",
    "        tf.keras.backend.clear_session()\n",
    "        tfds_train, tfds_val = get_batched_data(train_df, val_df)\n",
    "        hp = keras_tuner.HyperParameters()        \n",
    "        tuner = get_random_search_instance(objective, hp, train_df, season, False)\n",
    "        models = tuner.get_best_models()\n",
    "        best_model = models[0]\n",
    "        input_tensors ={}\n",
    "        for key,value in tfds_train.element_spec[0].items():\n",
    "            input_tensors[key] = tf.keras.layers.Input(type_spec = value)\n",
    "            shape = value.shape\n",
    "        best_model.call(input_tensors)\n",
    "        # best_model.build(shape)\n",
    "        # best_model.summary()\n",
    "        best_models.append(best_model)\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resume_tuning(objective, epochs=10):\n",
    "    for season, train_df, val_df in zip(NAME_SEASON, train_all_season, val_all_season):\n",
    "        tf.keras.backend.clear_session()    \n",
    "        tfds_train, tfds_val = get_batched_data(train_df, val_df)\n",
    "        hp = keras_tuner.HyperParameters()        \n",
    "        tuner = get_random_search_instance(objective, hp, train_df, season, False)\n",
    "        tuner.search(tfds_train, validation_data=tfds_val, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_objective = [\n",
    "    keras_tuner.Objective('val_output_Accuracy', \"max\"),\n",
    "    keras_tuner.Objective('val_output_Recall', \"max\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_fine_tune(finetune_objective, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resume_tuning(finetune_objective, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best results and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fttransformer.models.fttransformer import FTTransformer\n",
    "finetuned_recall_model_list = load_best_model(finetune_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_ft_tuned_list = generate_predict_list(finetuned_recall_model_list, test_dataset_input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_result(test_dataset_label_list, predict_ft_tuned_list)\n",
    "plot_feature_importance(test_dataset_label_list, predict_ft_tuned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_dataset_label_list, predict_ft_tuned_list):\n",
    "    metric_list = [metrics.Recall(name = 'Recall'), metrics.BinaryAccuracy(name = 'Accuracy')]\n",
    "    for real_labels, predict_labels in zip(test_dataset_label_list, predict_ft_tuned_list):\n",
    "        real_values = real_labels['is_fire'].values\n",
    "        predicted = np.array([1 if x>0.5 else 0 for x in predict_labels['output'].ravel()])\n",
    "        actual = np.array([x for x in real_values])\n",
    "        print(predicted)\n",
    "        print(real_values)\n",
    "        for m in metric_list:\n",
    "            m.update_state(actual, predicted)\n",
    "            print(f'{m.name}: {m.result()}')\n",
    "\n",
    "        conf_mat = confusion_matrix(actual, predicted)\n",
    "        displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        displ.plot()\n",
    "\n",
    "plot_confusion_matrix(test_dataset_label_list, predict_ft_tuned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
